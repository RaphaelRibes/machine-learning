{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "08c0a799-20c5-48c5-9ce4-51c9fbc582e9",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package wordnet to /home/tiziri-\n",
      "[nltk_data]     tamani/nltk_data...\n",
      "[nltk_data]   Package wordnet is already up-to-date!\n",
      "[nltk_data] Downloading package punkt to /home/tiziri-\n",
      "[nltk_data]     tamani/nltk_data...\n",
      "[nltk_data]   Package punkt is already up-to-date!\n",
      "[nltk_data] Downloading package averaged_perceptron_tagger to\n",
      "[nltk_data]     /home/tiziri-tamani/nltk_data...\n",
      "[nltk_data]   Package averaged_perceptron_tagger is already up-to-\n",
      "[nltk_data]       date!\n",
      "[nltk_data] Downloading package stopwords to /home/tiziri-\n",
      "[nltk_data]     tamani/nltk_data...\n",
      "[nltk_data]   Package stopwords is already up-to-date!\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 26,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "import sklearn as sk\n",
    "import nltk\n",
    "\n",
    "nltk.download('wordnet')\n",
    "nltk.download('punkt')\n",
    "nltk.download('averaged_perceptron_tagger')\n",
    "nltk.download('stopwords')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "3cea3488-85d9-4d76-9c3e-3c248486b7eb",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Unnamed: 0</th>\n",
       "      <th>tweet_id</th>\n",
       "      <th>text</th>\n",
       "      <th>science_related</th>\n",
       "      <th>scientific_claim</th>\n",
       "      <th>scientific_reference</th>\n",
       "      <th>scientific_context</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0</td>\n",
       "      <td>3,16669998137483E+017</td>\n",
       "      <td>Knees are a bit sore. i guess that's a sign th...</td>\n",
       "      <td>0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>1</td>\n",
       "      <td>3,19090866545386E+017</td>\n",
       "      <td>McDonald's breakfast stop then the gym üèÄüí™</td>\n",
       "      <td>0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>2</td>\n",
       "      <td>3,22030931022066E+017</td>\n",
       "      <td>Can any Gynecologist with Cancer Experience ex...</td>\n",
       "      <td>1</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>3</td>\n",
       "      <td>3,22694830620807E+017</td>\n",
       "      <td>Couch-lock highs lead to sleeping in the couch...</td>\n",
       "      <td>1</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>4</td>\n",
       "      <td>3,28524426658329E+017</td>\n",
       "      <td>Does daily routine help prevent problems with ...</td>\n",
       "      <td>1</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   Unnamed: 0               tweet_id  \\\n",
       "0           0  3,16669998137483E+017   \n",
       "1           1  3,19090866545386E+017   \n",
       "2           2  3,22030931022066E+017   \n",
       "3           3  3,22694830620807E+017   \n",
       "4           4  3,28524426658329E+017   \n",
       "\n",
       "                                                text  science_related  \\\n",
       "0  Knees are a bit sore. i guess that's a sign th...                0   \n",
       "1          McDonald's breakfast stop then the gym üèÄüí™                0   \n",
       "2  Can any Gynecologist with Cancer Experience ex...                1   \n",
       "3  Couch-lock highs lead to sleeping in the couch...                1   \n",
       "4  Does daily routine help prevent problems with ...                1   \n",
       "\n",
       "   scientific_claim  scientific_reference  scientific_context  \n",
       "0               0.0                   0.0                 0.0  \n",
       "1               0.0                   0.0                 0.0  \n",
       "2               1.0                   0.0                 0.0  \n",
       "3               1.0                   0.0                 0.0  \n",
       "4               1.0                   0.0                 0.0  "
      ]
     },
     "execution_count": 27,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df = pd.read_csv('scitweets_export.tsv', sep='\\t')\n",
    "df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "374c2105-0a68-4367-a12c-0204cf1c9d36",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "id": "4c3546df-1254-4caa-a3ec-a195ab833b6d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       1.00      0.95      0.98        64\n",
      "           1       0.96      1.00      0.98        73\n",
      "\n",
      "    accuracy                           0.98       137\n",
      "   macro avg       0.98      0.98      0.98       137\n",
      "weighted avg       0.98      0.98      0.98       137\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# ------------------------------------------------------------\n",
    "# √âTAPE 2 : {CLAIM, REF} vs {CONTEXT} (Classification binaire)\n",
    "# ------------------------------------------------------------\n",
    "\n",
    "# 1. Pr√©paration des donn√©es\n",
    "sci_df = df[df['science_related'] == 1].copy()\n",
    "\n",
    "# Nettoyage du texte SI NECESSAIRE\n",
    "if 'cleaned_text' not in sci_df.columns:\n",
    "    sci_df['cleaned_text'] = sci_df['text'].apply(preprocess_tweet)\n",
    "\n",
    "# Cr√©ation de la target\n",
    "sci_df['claim_or_ref'] = ((sci_df['scientific_claim'] == 1) | (sci_df['scientific_reference'] == 1)).astype(int)\n",
    "\n",
    "# 2. Vectorisation TF-IDF\n",
    "vectorizer = TfidfVectorizer(ngram_range=(1, 2), max_features=20000)\n",
    "X = vectorizer.fit_transform(sci_df['cleaned_text'])\n",
    "y = sci_df['claim_or_ref']\n",
    "\n",
    "# 3. √âquilibrage SMOTE\n",
    "smote = SMOTE(random_state=42)\n",
    "X_res, y_res = smote.fit_resample(X, y)\n",
    "\n",
    "# 4. Split des donn√©es\n",
    "X_train, X_test, y_train, y_test = train_test_split(X_res, y_res, test_size=0.2, random_state=42)\n",
    "\n",
    "# 5. Entra√Ænement des mod√®les (exemple avec Random Forest)\n",
    "model = RandomForestClassifier(random_state=42)\n",
    "model.fit(X_train, y_train)\n",
    "\n",
    "# 6. √âvaluation\n",
    "y_pred = model.predict(X_test)\n",
    "print(classification_report(y_test, y_pred))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "id": "2cd4fc3f-5ef8-482a-afad-4e173888a539",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "                                                text  \\\n",
      "2  Can any Gynecologist with Cancer Experience ex...   \n",
      "3  Couch-lock highs lead to sleeping in the couch...   \n",
      "4  Does daily routine help prevent problems with ...   \n",
      "6  ‚ÄúTraffic Jam‚Äù In Brain‚Äôs Neurons Could Be Caus...   \n",
      "7  Can playing more games improve lives and save ...   \n",
      "\n",
      "                                        cleaned_text  claim_or_ref  \n",
      "2  gynecologist cancer experience explain danger ...             1  \n",
      "3  couch-lock high lead sleeping couch . got ta s...             1  \n",
      "4  daily routine help prevent problem bipolar dis...             1  \n",
      "6  ‚Äú traffic jam ‚Äù brain ‚Äô neuron could cause sta...             1  \n",
      "7  playing game improve life save world ? @ chris...             1  \n"
     ]
    }
   ],
   "source": [
    "print(sci_df[['text', 'cleaned_text', 'claim_or_ref']].head())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "30cbdc7d-f3e1-432c-af49-1e53aa67e77f",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cc586db6-6e42-4495-bef4-27a87bee6a9a",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "42f28553-4858-4a2f-affd-c0923aad4293",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "id": "13f4e05e-2c81-46be-baa9-ccf1d2ece2a3",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Distribution des combinaisons :\n",
      "is_claim_or_ref  is_context\n",
      "1                1             218\n",
      "                 0             124\n",
      "0                1              33\n",
      "Name: count, dtype: int64\n",
      "\n",
      "============================================================\n",
      "Optimisation de SVM\n",
      "============================================================\n",
      "Fitting 10 folds for each of 6 candidates, totalling 60 fits\n",
      "\n",
      "Meilleurs param√®tres: {'estimator__C': 10, 'estimator__kernel': 'linear'}\n",
      "Accuracy (moyenne ¬± √©cart-type): 0.8094 ¬± 0.0427\n",
      "Scores par fold: [0.73684211 0.82894737 0.80263158 0.81578947 0.82894737 0.83783784\n",
      " 0.82432432 0.89189189 0.75675676 0.77027027]\n",
      "\n",
      "============================================================\n",
      "Optimisation de Naive Bayes\n",
      "============================================================\n",
      "Fitting 10 folds for each of 6 candidates, totalling 60 fits\n",
      "\n",
      "Meilleurs param√®tres: {'estimator__alpha': 0.5, 'estimator__fit_prior': True}\n",
      "Accuracy (moyenne ¬± √©cart-type): 0.8053 ¬± 0.0415\n",
      "Scores par fold: [0.75       0.81578947 0.81578947 0.78947368 0.86842105 0.7972973\n",
      " 0.81081081 0.87837838 0.78378378 0.74324324]\n",
      "\n",
      "============================================================\n",
      "Optimisation de k-NN\n",
      "============================================================\n",
      "Fitting 10 folds for each of 12 candidates, totalling 120 fits\n",
      "\n",
      "Meilleurs param√®tres: {'estimator__metric': 'euclidean', 'estimator__n_neighbors': 5, 'estimator__weights': 'distance'}\n",
      "Accuracy (moyenne ¬± √©cart-type): 0.8148 ¬± 0.0398\n",
      "Scores par fold: [0.78947368 0.78947368 0.81578947 0.77631579 0.85526316 0.78378378\n",
      " 0.89189189 0.86486486 0.77027027 0.81081081]\n",
      "\n",
      "============================================================\n",
      "Optimisation de Random Forest\n",
      "============================================================\n",
      "Fitting 10 folds for each of 12 candidates, totalling 120 fits\n",
      "\n",
      "Meilleurs param√®tres: {'estimator__max_depth': None, 'estimator__min_samples_split': 2, 'estimator__n_estimators': 100}\n",
      "Accuracy (moyenne ¬± √©cart-type): 0.8174 ¬± 0.0412\n",
      "Scores par fold: [0.76315789 0.82894737 0.81578947 0.81578947 0.82894737 0.85135135\n",
      " 0.82432432 0.90540541 0.75675676 0.78378378]\n",
      "\n",
      "============================================================\n",
      "Optimisation de Logistic Regression\n",
      "============================================================\n",
      "Fitting 10 folds for each of 6 candidates, totalling 60 fits\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/tiziri-tamani/anaconda3/lib/python3.12/site-packages/sklearn/svm/_base.py:1235: ConvergenceWarning: Liblinear failed to converge, increase the number of iterations.\n",
      "  warnings.warn(\n",
      "/home/tiziri-tamani/anaconda3/lib/python3.12/site-packages/sklearn/svm/_base.py:1235: ConvergenceWarning: Liblinear failed to converge, increase the number of iterations.\n",
      "  warnings.warn(\n",
      "/home/tiziri-tamani/anaconda3/lib/python3.12/site-packages/sklearn/svm/_base.py:1235: ConvergenceWarning: Liblinear failed to converge, increase the number of iterations.\n",
      "  warnings.warn(\n",
      "/home/tiziri-tamani/anaconda3/lib/python3.12/site-packages/sklearn/svm/_base.py:1235: ConvergenceWarning: Liblinear failed to converge, increase the number of iterations.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Meilleurs param√®tres: {'estimator__C': 10, 'estimator__penalty': 'l1', 'estimator__solver': 'liblinear'}\n",
      "Accuracy (moyenne ¬± √©cart-type): 0.8640 ¬± 0.0344\n",
      "Scores par fold: [0.80263158 0.90789474 0.86842105 0.85526316 0.86842105 0.89189189\n",
      " 0.86486486 0.91891892 0.82432432 0.83783784]\n",
      "\n",
      "============================================================\n",
      "Optimisation de Gradient Boosting\n",
      "============================================================\n",
      "Fitting 10 folds for each of 8 candidates, totalling 80 fits\n",
      "\n",
      "Meilleurs param√®tres: {'estimator__learning_rate': 0.1, 'estimator__max_depth': 3, 'estimator__n_estimators': 50}\n",
      "Accuracy (moyenne ¬± √©cart-type): 0.8721 ¬± 0.0489\n",
      "Scores par fold: [0.80263158 0.90789474 0.88157895 0.85526316 0.88157895 0.93243243\n",
      " 0.82432432 0.95945946 0.81081081 0.86486486]\n"
     ]
    }
   ],
   "source": [
    "# ------------------------------------------------------------\n",
    "# √âTAPE 2 : Classification Binaire Multi-Label - sans equilibre les calsse \n",
    "# ------------------------------------------------------------\n",
    "\n",
    "from sklearn.model_selection import KFold\n",
    "from sklearn.metrics import make_scorer\n",
    "import numpy as np\n",
    "from sklearn.ensemble import GradientBoostingClassifier\n",
    "from sklearn.model_selection import KFold\n",
    "from sklearn.metrics import make_scorer, f1_score\n",
    "from imblearn.ensemble import BalancedRandomForestClassifier\n",
    "from iterstrat.ml_stratifiers import MultilabelStratifiedKFold\n",
    "from sklearn.multioutput import MultiOutputClassifier\n",
    "\n",
    "\n",
    "stop_words = set(stopwords.words('english'))\n",
    "custom_stop_words = {\"http\", \"https\", \"rt\", \"co\", \"amp\", \"via\"}\n",
    "stop_words.update(custom_stop_words)\n",
    "lemmatizer = WordNetLemmatizer()\n",
    "\n",
    "def preprocess_tweet(tweet):\n",
    "    # 1. Supprime mentions (@) et caract√®res sp√©ciaux\n",
    "    #tweet = re.sub(r\"@\\w+|\\W\", \" \", tweet)\n",
    "    # 2. Convertit en minuscules\n",
    "    tweet = tweet.lower()\n",
    "    # 3. Tokenization et lemmatisation\n",
    "    tokens = word_tokenize(tweet)\n",
    "    tokens = [lemmatizer.lemmatize(word) for word in tokens if word not in stop_words]\n",
    "    # 4. Recombine en texte\n",
    "    return \" \".join(tokens)\n",
    "\n",
    "\n",
    "# 1. Pr√©paration des donn√©es (tweets scientifiques uniquement)\n",
    "sci_df = df[df['science_related'] == 1].copy()\n",
    "\n",
    "# 2. Cr√©ation des DEUX colonnes cibles (peuvent valoir 1 simultan√©ment)\n",
    "sci_df['is_claim_or_ref'] = ((sci_df['scientific_claim'] == 1) | (sci_df['scientific_reference'] == 1)).astype(int)\n",
    "sci_df['is_context'] = (sci_df['scientific_context'] == 1).astype(int)\n",
    "\n",
    "# Afficher les combinaisons possibles\n",
    "print(\"Distribution des combinaisons :\")\n",
    "print(sci_df[['is_claim_or_ref', 'is_context']].value_counts())\n",
    "\n",
    "# 3. Pr√©traitement du texte (si pas d√©j√† fait)\n",
    "if 'cleaned_text' not in sci_df.columns:\n",
    "    sci_df['cleaned_text'] = sci_df['text'].apply(preprocess_tweet)\n",
    "\n",
    "# 4. Vectorisation TF-IDF\n",
    "vectorizer = TfidfVectorizer(ngram_range=(1, 3), max_features=20000)\n",
    "X = vectorizer.fit_transform(sci_df['cleaned_text'])\n",
    "\n",
    "# 5. Cr√©ation des cibles multi-labels\n",
    "y = sci_df[['is_claim_or_ref', 'is_context']].values\n",
    "\n",
    "\n",
    "# Fonction de scoring personnalis√©e pour multi-label\n",
    "def multilabel_accuracy(y_true, y_pred):\n",
    "    return np.mean(y_true == y_pred)\n",
    "\n",
    "# Configuration KFold\n",
    "kf = KFold(n_splits=10, shuffle=True, random_state=42)\n",
    "scorer = make_scorer(multilabel_accuracy)\n",
    "\n",
    "# Dictionnaire des mod√®les r√©vis√©\n",
    "models = {\n",
    "    \"SVM\": {\n",
    "        \"model\": SVC(probability=True),\n",
    "        \"params\": {\n",
    "            'estimator__C': [0.1, 1, 10],\n",
    "            'estimator__kernel': ['linear', 'rbf']\n",
    "        }\n",
    "    },\n",
    "    \"Naive Bayes\": {\n",
    "        \"model\": MultinomialNB(),\n",
    "        \"params\": {\n",
    "            'estimator__alpha': [0.1, 0.5, 1.0],\n",
    "            'estimator__fit_prior': [True, False]\n",
    "        }\n",
    "    },\n",
    "    \"k-NN\": {\n",
    "        \"model\": KNeighborsClassifier(),\n",
    "        \"params\": {\n",
    "            'estimator__n_neighbors': [3, 5, 7],\n",
    "            'estimator__weights': ['uniform', 'distance'],\n",
    "            'estimator__metric': ['euclidean', 'cosine']\n",
    "        }\n",
    "    },\n",
    "    \"Random Forest\": {\n",
    "        \"model\": RandomForestClassifier(random_state=42),\n",
    "        \"params\": {\n",
    "            'estimator__n_estimators': [50, 100],\n",
    "            'estimator__max_depth': [None, 10, 20],\n",
    "            'estimator__min_samples_split': [2, 5]\n",
    "        }\n",
    "    },\n",
    "    \"Logistic Regression\": {\n",
    "        \"model\": LogisticRegression(max_iter=1000),\n",
    "        \"params\": {\n",
    "            'estimator__C': [0.1, 1, 10],\n",
    "            'estimator__penalty': ['l1', 'l2'],\n",
    "            'estimator__solver': ['liblinear']\n",
    "        }\n",
    "    },\n",
    "    \"Gradient Boosting\": {\n",
    "        \"model\": GradientBoostingClassifier(random_state=42),\n",
    "        \"params\": {\n",
    "            'estimator__n_estimators': [50, 100],\n",
    "            'estimator__learning_rate': [0.01, 0.1],\n",
    "            'estimator__max_depth': [3, 5]\n",
    "        }\n",
    "    }\n",
    "}\n",
    "\n",
    "# √âvaluation des mod√®les\n",
    "for name, config in models.items():\n",
    "    print(f\"\\n{'='*60}\")\n",
    "    print(f\"Optimisation de {name}\")\n",
    "    print(f\"{'='*60}\")\n",
    "    \n",
    "    # Cr√©ation du pipeline\n",
    "    mo = MultiOutputClassifier(config['model'])\n",
    "    \n",
    "    # GridSearch avec validation crois√©e interne\n",
    "    grid = GridSearchCV(\n",
    "        estimator=mo,\n",
    "        param_grid=config['params'],\n",
    "        cv=10,\n",
    "        scoring=scorer,\n",
    "        n_jobs=-1,\n",
    "        verbose=1\n",
    "    )\n",
    "    \n",
    "    # IMPORTANT: On doit fit le GridSearch avant d'acc√©der aux best_params_\n",
    "    grid.fit(X, y)  # <-- Cette ligne √©tait manquante\n",
    "    \n",
    "    # Cross-validation externe\n",
    "    cv_scores = cross_val_score(\n",
    "        grid.best_estimator_,\n",
    "        X,\n",
    "        y,\n",
    "        cv=kf,\n",
    "        scoring=scorer,\n",
    "        n_jobs=-1\n",
    "    )\n",
    "    \n",
    "    print(f\"\\nMeilleurs param√®tres: {grid.best_params_}\")\n",
    "    print(f\"Accuracy (moyenne ¬± √©cart-type): {cv_scores.mean():.4f} ¬± {cv_scores.std():.4f}\")\n",
    "    print(f\"Scores par fold: {cv_scores}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3e7aef34-b823-49c9-ba22-5cfd9ae0a927",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3743ae31-a2ac-4559-99ef-4c7f3e61b691",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "954b42bc-2518-498a-81ea-09725762efa7",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "id": "59ca021e-5b64-4b35-93fc-d610ed4d0a86",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "============================================================\n",
      "Optimisation de SVM\n",
      "============================================================\n",
      "Fitting 10 folds for each of 6 candidates, totalling 60 fits\n",
      "\n",
      "Rapport de classification:\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "claim_or_ref       0.99      1.00      0.99       342\n",
      "     context       0.99      1.00      0.99       251\n",
      "\n",
      "   micro avg       0.99      1.00      0.99       593\n",
      "   macro avg       0.99      1.00      0.99       593\n",
      "weighted avg       0.99      1.00      0.99       593\n",
      " samples avg       0.99      1.00      0.99       593\n",
      "\n",
      "\n",
      "CV Accuracy: 0.9183 ¬± 0.0193\n",
      "Meilleurs param√®tres: {'estimator__C': 10, 'estimator__kernel': 'rbf'}\n",
      "\n",
      "============================================================\n",
      "Optimisation de Naive Bayes\n",
      "============================================================\n",
      "Fitting 10 folds for each of 6 candidates, totalling 60 fits\n",
      "\n",
      "Rapport de classification:\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "claim_or_ref       0.98      0.94      0.96       342\n",
      "     context       0.87      0.92      0.89       251\n",
      "\n",
      "   micro avg       0.93      0.93      0.93       593\n",
      "   macro avg       0.93      0.93      0.93       593\n",
      "weighted avg       0.93      0.93      0.93       593\n",
      " samples avg       0.94      0.94      0.93       593\n",
      "\n",
      "\n",
      "CV Accuracy: 0.8647 ¬± 0.0292\n",
      "Meilleurs param√®tres: {'estimator__alpha': 0.1, 'estimator__fit_prior': True}\n",
      "\n",
      "============================================================\n",
      "Optimisation de k-NN\n",
      "============================================================\n",
      "Fitting 10 folds for each of 12 candidates, totalling 120 fits\n",
      "\n",
      "Rapport de classification:\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "claim_or_ref       0.99      1.00      0.99       342\n",
      "     context       0.99      0.99      0.99       251\n",
      "\n",
      "   micro avg       0.99      0.99      0.99       593\n",
      "   macro avg       0.99      0.99      0.99       593\n",
      "weighted avg       0.99      0.99      0.99       593\n",
      " samples avg       0.99      1.00      0.99       593\n",
      "\n",
      "\n",
      "CV Accuracy: 0.8876 ¬± 0.0183\n",
      "Meilleurs param√®tres: {'estimator__metric': 'cosine', 'estimator__n_neighbors': 3, 'estimator__weights': 'distance'}\n",
      "\n",
      "============================================================\n",
      "Optimisation de Random Forest\n",
      "============================================================\n",
      "Fitting 10 folds for each of 12 candidates, totalling 120 fits\n",
      "\n",
      "Rapport de classification:\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "claim_or_ref       0.99      1.00      0.99       342\n",
      "     context       0.99      1.00      0.99       251\n",
      "\n",
      "   micro avg       0.99      1.00      0.99       593\n",
      "   macro avg       0.99      1.00      0.99       593\n",
      "weighted avg       0.99      1.00      0.99       593\n",
      " samples avg       0.99      1.00      0.99       593\n",
      "\n",
      "\n",
      "CV Accuracy: 0.9359 ¬± 0.0162\n",
      "Meilleurs param√®tres: {'estimator__max_depth': None, 'estimator__min_samples_split': 2, 'estimator__n_estimators': 100}\n",
      "\n",
      "============================================================\n",
      "Optimisation de Logistic Regression\n",
      "============================================================\n",
      "Fitting 10 folds for each of 6 candidates, totalling 60 fits\n",
      "\n",
      "Rapport de classification:\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "claim_or_ref       0.99      0.99      0.99       342\n",
      "     context       0.97      0.95      0.96       251\n",
      "\n",
      "   micro avg       0.98      0.97      0.97       593\n",
      "   macro avg       0.98      0.97      0.97       593\n",
      "weighted avg       0.98      0.97      0.97       593\n",
      " samples avg       0.98      0.98      0.97       593\n",
      "\n",
      "\n",
      "CV Accuracy: 0.8961 ¬± 0.0232\n",
      "Meilleurs param√®tres: {'estimator__C': 10, 'estimator__penalty': 'l1', 'estimator__solver': 'liblinear'}\n",
      "\n",
      "============================================================\n",
      "Optimisation de Gradient Boosting\n",
      "============================================================\n",
      "Fitting 10 folds for each of 8 candidates, totalling 80 fits\n",
      "\n",
      "Rapport de classification:\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "claim_or_ref       0.99      1.00      0.99       342\n",
      "     context       0.98      1.00      0.99       251\n",
      "\n",
      "   micro avg       0.99      1.00      0.99       593\n",
      "   macro avg       0.98      1.00      0.99       593\n",
      "weighted avg       0.99      1.00      0.99       593\n",
      " samples avg       0.99      1.00      0.99       593\n",
      "\n",
      "\n",
      "CV Accuracy: 0.9236 ¬± 0.0162\n",
      "Meilleurs param√®tres: {'estimator__learning_rate': 0.1, 'estimator__max_depth': 5, 'estimator__n_estimators': 100}\n"
     ]
    }
   ],
   "source": [
    "\n",
    "\"\"\"\n",
    "Ce script impl√©mente une classification multi-label pour cat√©goriser des tweets scientifiques selon deux dimensions :\n",
    "1) Claim/Reference (affirmation ou r√©f√©rence scientifique)\n",
    "2) Context (contexte scientifique)\n",
    "\n",
    "FONCTIONNEMENT PRINCIPAL :\n",
    "\n",
    "1. PR√âTRAITEMENT DES DONN√âES :\n",
    "   - Nettoie le texte (minuscules, suppression stopwords, lemmatisation)\n",
    "   - Filtre les tweets scientifiques (science_related == 1)\n",
    "   - Cr√©e deux labels binaires :\n",
    "     * is_claim_or_ref: 1 si le tweet contient une affirmation ou r√©f√©rence scientifique\n",
    "     * is_context: 1 si le tweet fournit un contexte scientifique\n",
    "\n",
    "2. VECTORISATION :\n",
    "   - Convertit le texte en features num√©riques via TF-IDF\n",
    "   - Utilise des uni+bigrammes avec seuils min/max de fr√©quence\n",
    "   - Limite √† 10 000 features maximum pour √©viter la mal√©diction de la dimension\n",
    "\n",
    "3. √âQUILIBRAGE DES DONN√âES :\n",
    "   - Impl√©mente un r√©√©chantillonnage manuel sp√©cifique au multi-label :\n",
    "     * Identifie toutes les combinaisons de labels possibles\n",
    "     * Sur√©chantillonne les combinaisons sous-repr√©sent√©es\n",
    "   - Permet de g√©rer les d√©s√©quilibres entre classes\n",
    "\n",
    "4. MOD√âLISATION :\n",
    "   - Teste plusieurs algorithmes classiques (SVM, RandomForest, Logistic Regression etc.)\n",
    "   - Chaque mod√®le est encapsul√© dans un MultiOutputClassifier pour g√©rer le multi-label\n",
    "   - Utilise GridSearchCV pour optimiser les hyperparam√®tres\n",
    "\n",
    "5. √âVALUATION :\n",
    "   - Validation crois√©e (10 folds) pour estimer la performance g√©n√©ralisable\n",
    "   - Mesure l'accuracy globale et par label\n",
    "   - G√©n√®re des rapports d√©taill√©s (precision, recall, f1-score)\n",
    "\n",
    "CARACT√âRISTIQUES CL√âS :\n",
    "- G√®re les tweets pouvant appartenir √† plusieurs cat√©gories simultan√©ment\n",
    "- Pr√©serve les relations entre labels pendant l'√©quilibrage\n",
    "- √âvite le surapprentissage par des param√®tres conservateurs (max_df, min_df)\n",
    "- Permet de comparer objectivement plusieurs algorithmes\n",
    "\n",
    "UTILISATION TYPIQUE :\n",
    "1. Charger un DataFrame pandas contenant les tweets et labels\n",
    "2. Ex√©cuter le script pour entra√Æner et √©valuer les mod√®les\n",
    "3. Analyser les rapports de classification pour s√©lectionner le meilleur mod√®le\n",
    "\"\"\"\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "# ------------------------------------------------------------\n",
    "# SOLUTION FINALE CORRIG√âE - CLASSIFICATION MULTI-LABEL\n",
    "# Avec √©quilibrage personnalis√© et validation KFold\n",
    "# ------------------------------------------------------------\n",
    "\n",
    "import numpy as np\n",
    "from sklearn.model_selection import KFold\n",
    "from sklearn.metrics import make_scorer, classification_report\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from sklearn.svm import SVC\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.multioutput import MultiOutputClassifier\n",
    "from sklearn.model_selection import GridSearchCV\n",
    "from scipy.sparse import vstack\n",
    "\n",
    "# 1. Pr√©paration des donn√©es\n",
    "stop_words = set(stopwords.words('english'))\n",
    "custom_stop_words = {\"http\", \"https\", \"rt\", \"co\", \"amp\", \"via\"}\n",
    "stop_words.update(custom_stop_words)\n",
    "lemmatizer = WordNetLemmatizer()\n",
    "\n",
    "def preprocess_tweet(tweet):\n",
    "    tweet = tweet.lower()\n",
    "    tokens = word_tokenize(tweet)\n",
    "    tokens = [lemmatizer.lemmatize(word) for word in tokens if word not in stop_words]\n",
    "    return \" \".join(tokens)\n",
    "\n",
    "# 2. Chargement et pr√©paration\n",
    "sci_df = df[df['science_related'] == 1].copy()\n",
    "sci_df['is_claim_or_ref'] = ((sci_df['scientific_claim'] == 1) | (sci_df['scientific_reference'] == 1))\n",
    "sci_df['is_context'] = (sci_df['scientific_context'] == 1)\n",
    "sci_df['cleaned_text'] = sci_df['text'].apply(preprocess_tweet)\n",
    "\n",
    "# 3. Vectorisation\n",
    "vectorizer = TfidfVectorizer(\n",
    "    ngram_range=(1, 2),  # R√©duit √† bigrammes pour plus de stabilit√©\n",
    "    max_features=10000,\n",
    "    min_df=5,\n",
    "    max_df=0.85\n",
    ")\n",
    "X = vectorizer.fit_transform(sci_df['cleaned_text'])\n",
    "y = sci_df[['is_claim_or_ref', 'is_context']].values\n",
    "\n",
    "# 4. R√©√©chantillonnage manuel pour multi-label\n",
    "def multilabel_oversample(X, y, random_state=None):\n",
    "    np.random.seed(random_state)\n",
    "    \n",
    "    # Compter les occurrences de chaque combinaison de labels\n",
    "    unique_labels, counts = np.unique(y, axis=0, return_counts=True)\n",
    "    max_count = max(counts)\n",
    "    \n",
    "    resampled_X = []\n",
    "    resampled_y = []\n",
    "    \n",
    "    for label_combination in unique_labels:\n",
    "        indices = np.where((y == label_combination).all(axis=1))[0]\n",
    "        n_samples = len(indices)\n",
    "        \n",
    "        # Sur√©chantillonnage seulement pour les classes minoritaires\n",
    "        if n_samples < max_count:\n",
    "            n_to_add = max_count - n_samples\n",
    "            selected = np.random.choice(indices, size=n_to_add, replace=True)\n",
    "            \n",
    "            resampled_X.append(X[selected])\n",
    "            resampled_y.append(y[selected])\n",
    "    \n",
    "    if resampled_X:\n",
    "        return vstack([X] + resampled_X), np.vstack([y] + resampled_y)\n",
    "    return X, y\n",
    "\n",
    "X_res, y_res = multilabel_oversample(X, y, random_state=42)\n",
    "\n",
    "# 5. Configuration des mod√®les avec class_weight\n",
    "models = {\n",
    "    \"SVM\": {\n",
    "        \"model\": SVC(probability=True),\n",
    "        \"params\": {\n",
    "            'estimator__C': [0.1, 1, 10],\n",
    "            'estimator__kernel': ['linear', 'rbf']\n",
    "        }\n",
    "    },\n",
    "    \"Naive Bayes\": {\n",
    "        \"model\": MultinomialNB(),\n",
    "        \"params\": {\n",
    "            'estimator__alpha': [0.1, 0.5, 1.0],\n",
    "            'estimator__fit_prior': [True, False]\n",
    "        }\n",
    "    },\n",
    "    \"k-NN\": {\n",
    "        \"model\": KNeighborsClassifier(),\n",
    "        \"params\": {\n",
    "            'estimator__n_neighbors': [3, 5, 7],\n",
    "            'estimator__weights': ['uniform', 'distance'],\n",
    "            'estimator__metric': ['euclidean', 'cosine']\n",
    "        }\n",
    "    },\n",
    "    \"Random Forest\": {\n",
    "        \"model\": RandomForestClassifier(random_state=42),\n",
    "        \"params\": {\n",
    "            'estimator__n_estimators': [50, 100],\n",
    "            'estimator__max_depth': [None, 10, 20],\n",
    "            'estimator__min_samples_split': [2, 5]\n",
    "        }\n",
    "    },\n",
    "    \"Logistic Regression\": {\n",
    "        \"model\": LogisticRegression(max_iter=1000),\n",
    "        \"params\": {\n",
    "            'estimator__C': [0.1, 1, 10],\n",
    "            'estimator__penalty': ['l1', 'l2'],\n",
    "            'estimator__solver': ['liblinear']\n",
    "        }\n",
    "    },\n",
    "    \"Gradient Boosting\": {\n",
    "        \"model\": GradientBoostingClassifier(random_state=42),\n",
    "        \"params\": {\n",
    "            'estimator__n_estimators': [50, 100],\n",
    "            'estimator__learning_rate': [0.01, 0.1],\n",
    "            'estimator__max_depth': [3, 5]\n",
    "        }\n",
    "    }\n",
    "}\n",
    "\n",
    "# 6. √âvaluation avec KFold\n",
    "kf = KFold(n_splits=10, shuffle=True, random_state=42)\n",
    "scorer = make_scorer(lambda y_true, y_pred: np.mean(y_true == y_pred))\n",
    "\n",
    "for name, config in models.items():\n",
    "    print(f\"\\n{'='*60}\")\n",
    "    print(f\"Optimisation de {name}\")\n",
    "    print(f\"{'='*60}\")\n",
    "    \n",
    "    model = MultiOutputClassifier(config['model'])\n",
    "    \n",
    "    grid = GridSearchCV(\n",
    "        model,\n",
    "        param_grid=config['params'],\n",
    "        cv=kf,\n",
    "        scoring=scorer,\n",
    "        n_jobs=-1,\n",
    "        verbose=1\n",
    "    )\n",
    "    \n",
    "    grid.fit(X_res, y_res)\n",
    "    \n",
    "    # √âvaluation\n",
    "    y_pred = grid.predict(X)\n",
    "    print(\"\\nRapport de classification:\")\n",
    "    print(classification_report(y, y_pred, target_names=['claim_or_ref', 'context']))\n",
    "    \n",
    "    # Validation crois√©e\n",
    "    cv_scores = cross_val_score(grid.best_estimator_, X_res, y_res, cv=kf, scoring=scorer)\n",
    "    print(f\"\\nCV Accuracy: {np.mean(cv_scores):.4f} ¬± {np.std(cv_scores):.4f}\")\n",
    "    print(f\"Meilleurs param√®tres: {grid.best_params_}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "dd464d26-d43b-4e40-97e5-b2f1fe737e2a",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "db4eec51-2ae7-41e5-8b87-c5dae571d9fa",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
