{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "08c0a799-20c5-48c5-9ce4-51c9fbc582e9",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package wordnet to /home/tiziri-\n",
      "[nltk_data]     tamani/nltk_data...\n",
      "[nltk_data]   Package wordnet is already up-to-date!\n",
      "[nltk_data] Downloading package punkt to /home/tiziri-\n",
      "[nltk_data]     tamani/nltk_data...\n",
      "[nltk_data]   Package punkt is already up-to-date!\n",
      "[nltk_data] Downloading package averaged_perceptron_tagger to\n",
      "[nltk_data]     /home/tiziri-tamani/nltk_data...\n",
      "[nltk_data]   Package averaged_perceptron_tagger is already up-to-\n",
      "[nltk_data]       date!\n",
      "[nltk_data] Downloading package stopwords to /home/tiziri-\n",
      "[nltk_data]     tamani/nltk_data...\n",
      "[nltk_data]   Package stopwords is already up-to-date!\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "import sklearn as sk\n",
    "import nltk\n",
    "\n",
    "nltk.download('wordnet')\n",
    "nltk.download('punkt')\n",
    "nltk.download('averaged_perceptron_tagger')\n",
    "nltk.download('stopwords')\n",
    "\n",
    "from nltk.corpus import stopwords\n",
    "import pandas as pd\n",
    "import re\n",
    "from nltk.corpus import stopwords\n",
    "from nltk.tokenize import word_tokenize\n",
    "from nltk.stem import WordNetLemmatizer\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "from sklearn.model_selection import train_test_split, GridSearchCV, KFold, cross_val_score\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.naive_bayes import MultinomialNB\n",
    "from sklearn.neighbors import KNeighborsClassifier\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from sklearn.svm import SVC\n",
    "from sklearn.metrics import classification_report\n",
    "from sklearn.decomposition import PCA\n",
    "import matplotlib.pyplot as plt\n",
    "from imblearn.over_sampling import SMOTE\n",
    "import nltk"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "3cea3488-85d9-4d76-9c3e-3c248486b7eb",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Unnamed: 0</th>\n",
       "      <th>tweet_id</th>\n",
       "      <th>text</th>\n",
       "      <th>science_related</th>\n",
       "      <th>scientific_claim</th>\n",
       "      <th>scientific_reference</th>\n",
       "      <th>scientific_context</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0</td>\n",
       "      <td>3,16669998137483E+017</td>\n",
       "      <td>Knees are a bit sore. i guess that's a sign th...</td>\n",
       "      <td>0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>1</td>\n",
       "      <td>3,19090866545386E+017</td>\n",
       "      <td>McDonald's breakfast stop then the gym üèÄüí™</td>\n",
       "      <td>0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>2</td>\n",
       "      <td>3,22030931022066E+017</td>\n",
       "      <td>Can any Gynecologist with Cancer Experience ex...</td>\n",
       "      <td>1</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>3</td>\n",
       "      <td>3,22694830620807E+017</td>\n",
       "      <td>Couch-lock highs lead to sleeping in the couch...</td>\n",
       "      <td>1</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>4</td>\n",
       "      <td>3,28524426658329E+017</td>\n",
       "      <td>Does daily routine help prevent problems with ...</td>\n",
       "      <td>1</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   Unnamed: 0               tweet_id  \\\n",
       "0           0  3,16669998137483E+017   \n",
       "1           1  3,19090866545386E+017   \n",
       "2           2  3,22030931022066E+017   \n",
       "3           3  3,22694830620807E+017   \n",
       "4           4  3,28524426658329E+017   \n",
       "\n",
       "                                                text  science_related  \\\n",
       "0  Knees are a bit sore. i guess that's a sign th...                0   \n",
       "1          McDonald's breakfast stop then the gym üèÄüí™                0   \n",
       "2  Can any Gynecologist with Cancer Experience ex...                1   \n",
       "3  Couch-lock highs lead to sleeping in the couch...                1   \n",
       "4  Does daily routine help prevent problems with ...                1   \n",
       "\n",
       "   scientific_claim  scientific_reference  scientific_context  \n",
       "0               0.0                   0.0                 0.0  \n",
       "1               0.0                   0.0                 0.0  \n",
       "2               1.0                   0.0                 0.0  \n",
       "3               1.0                   0.0                 0.0  \n",
       "4               1.0                   0.0                 0.0  "
      ]
     },
     "execution_count": 23,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df = pd.read_csv('scitweets_export.tsv', sep='\\t')\n",
    "df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "374c2105-0a68-4367-a12c-0204cf1c9d36",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package stopwords to /home/tiziri-\n",
      "[nltk_data]     tamani/nltk_data...\n",
      "[nltk_data]   Package stopwords is already up-to-date!\n",
      "[nltk_data] Downloading package punkt to /home/tiziri-\n",
      "[nltk_data]     tamani/nltk_data...\n",
      "[nltk_data]   Package punkt is already up-to-date!\n",
      "[nltk_data] Downloading package wordnet to /home/tiziri-\n",
      "[nltk_data]     tamani/nltk_data...\n",
      "[nltk_data]   Package wordnet is already up-to-date!\n"
     ]
    }
   ],
   "source": [
    "from nltk.corpus import stopwords\n",
    "from nltk.tokenize import word_tokenize\n",
    "from nltk.stem import WordNetLemmatizer\n",
    "import re\n",
    "\n",
    "# Initialisation (√† mettre en d√©but de notebook)\n",
    "nltk.download('stopwords')\n",
    "nltk.download('punkt')\n",
    "nltk.download('wordnet')\n",
    "\n",
    "stop_words = set(stopwords.words('english'))\n",
    "custom_stop_words = {\"http\", \"https\", \"rt\", \"co\", \"amp\", \"via\"}\n",
    "stop_words.update(custom_stop_words)\n",
    "lemmatizer = WordNetLemmatizer()\n",
    "\n",
    "def preprocess_tweet(tweet):\n",
    "    # 1. Supprime mentions (@) et caract√®res sp√©ciaux\n",
    "    tweet = re.sub(r\"@\\w+|\\W\", \" \", tweet)\n",
    "    # 2. Convertit en minuscules\n",
    "    tweet = tweet.lower()\n",
    "    # 3. Tokenization et lemmatisation\n",
    "    tokens = word_tokenize(tweet)\n",
    "    tokens = [lemmatizer.lemmatize(word) for word in tokens if word not in stop_words]\n",
    "    # 4. Recombine en texte\n",
    "    return \" \".join(tokens)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "4c3546df-1254-4caa-a3ec-a195ab833b6d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       1.00      0.94      0.97        64\n",
      "           1       0.95      1.00      0.97        73\n",
      "\n",
      "    accuracy                           0.97       137\n",
      "   macro avg       0.97      0.97      0.97       137\n",
      "weighted avg       0.97      0.97      0.97       137\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# ------------------------------------------------------------\n",
    "# √âTAPE 2 : {CLAIM, REF} vs {CONTEXT} (Classification binaire)\n",
    "# ------------------------------------------------------------\n",
    "\n",
    "# 1. Pr√©paration des donn√©es\n",
    "sci_df = df[df['science_related'] == 1].copy()\n",
    "\n",
    "# Nettoyage du texte SI NECESSAIRE\n",
    "if 'cleaned_text' not in sci_df.columns:\n",
    "    sci_df['cleaned_text'] = sci_df['text'].apply(preprocess_tweet)\n",
    "\n",
    "# Cr√©ation de la target\n",
    "sci_df['claim_or_ref'] = ((sci_df['scientific_claim'] == 1) | (sci_df['scientific_reference'] == 1)).astype(int)\n",
    "\n",
    "# 2. Vectorisation TF-IDF\n",
    "vectorizer = TfidfVectorizer(ngram_range=(1, 2), max_features=20000)\n",
    "X = vectorizer.fit_transform(sci_df['cleaned_text'])\n",
    "y = sci_df['claim_or_ref']\n",
    "\n",
    "# 3. √âquilibrage SMOTE\n",
    "smote = SMOTE(random_state=42)\n",
    "X_res, y_res = smote.fit_resample(X, y)\n",
    "\n",
    "# 4. Split des donn√©es\n",
    "X_train, X_test, y_train, y_test = train_test_split(X_res, y_res, test_size=0.2, random_state=42)\n",
    "\n",
    "# 5. Entra√Ænement des mod√®les (exemple avec Random Forest)\n",
    "model = RandomForestClassifier(random_state=42)\n",
    "model.fit(X_train, y_train)\n",
    "\n",
    "# 6. √âvaluation\n",
    "y_pred = model.predict(X_test)\n",
    "print(classification_report(y_test, y_pred))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "2cd4fc3f-5ef8-482a-afad-4e173888a539",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "                                                text  \\\n",
      "2  Can any Gynecologist with Cancer Experience ex...   \n",
      "3  Couch-lock highs lead to sleeping in the couch...   \n",
      "4  Does daily routine help prevent problems with ...   \n",
      "6  ‚ÄúTraffic Jam‚Äù In Brain‚Äôs Neurons Could Be Caus...   \n",
      "7  Can playing more games improve lives and save ...   \n",
      "\n",
      "                                        cleaned_text  claim_or_ref  \n",
      "2  gynecologist cancer experience explain danger ...             1  \n",
      "3  couch lock high lead sleeping couch got ta sto...             1  \n",
      "4  daily routine help prevent problem bipolar dis...             1  \n",
      "6  traffic jam brain neuron could cause statin re...             1  \n",
      "7  playing game improve life save world uetvu099a...             1  \n"
     ]
    }
   ],
   "source": [
    "print(sci_df[['text', 'cleaned_text', 'claim_or_ref']].head())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "30cbdc7d-f3e1-432c-af49-1e53aa67e77f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Distribution des combinaisons :\n",
      "is_claim_or_ref  is_context\n",
      "1                1             218\n",
      "                 0             124\n",
      "0                1              33\n",
      "Name: count, dtype: int64\n",
      "\n",
      "Rapport de classification pour chaque label :\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "   CLAIM/REF       0.91      1.00      0.95       342\n",
      "     CONTEXT       0.75      0.97      0.85       251\n",
      "\n",
      "   micro avg       0.84      0.99      0.91       593\n",
      "   macro avg       0.83      0.99      0.90       593\n",
      "weighted avg       0.84      0.99      0.91       593\n",
      " samples avg       0.85      0.99      0.89       593\n",
      "\n",
      "\n",
      "Accuracy stricte (exact match) : 0.6880\n"
     ]
    }
   ],
   "source": [
    "# ------------------------------------------------------------\n",
    "# √âTAPE 2 : Classification Binaire Multi-Label {CLAIM/REF} et {CONTEXT}\n",
    "# ------------------------------------------------------------\n",
    "\n",
    "stop_words = set(stopwords.words('english'))\n",
    "custom_stop_words = {\"http\", \"https\", \"rt\", \"co\", \"amp\", \"via\"}\n",
    "stop_words.update(custom_stop_words)\n",
    "lemmatizer = WordNetLemmatizer()\n",
    "\n",
    "def preprocess_tweet(tweet):\n",
    "    # 1. Supprime mentions (@) et caract√®res sp√©ciaux\n",
    "    #tweet = re.sub(r\"@\\w+|\\W\", \" \", tweet)\n",
    "    # 2. Convertit en minuscules\n",
    "    tweet = tweet.lower()\n",
    "    # 3. Tokenization et lemmatisation\n",
    "    tokens = word_tokenize(tweet)\n",
    "    tokens = [lemmatizer.lemmatize(word) for word in tokens if word not in stop_words]\n",
    "    # 4. Recombine en texte\n",
    "    return \" \".join(tokens)\n",
    "\n",
    "\n",
    "# 1. Pr√©paration des donn√©es (tweets scientifiques uniquement)\n",
    "sci_df = df[df['science_related'] == 1].copy()\n",
    "\n",
    "# 2. Cr√©ation des DEUX colonnes cibles (peuvent valoir 1 simultan√©ment)\n",
    "sci_df['is_claim_or_ref'] = ((sci_df['scientific_claim'] == 1) | (sci_df['scientific_reference'] == 1)).astype(int)\n",
    "sci_df['is_context'] = (sci_df['scientific_context'] == 1).astype(int)\n",
    "\n",
    "# Afficher les combinaisons possibles\n",
    "print(\"Distribution des combinaisons :\")\n",
    "print(sci_df[['is_claim_or_ref', 'is_context']].value_counts())\n",
    "\n",
    "# 3. Pr√©traitement du texte (si pas d√©j√† fait)\n",
    "if 'cleaned_text' not in sci_df.columns:\n",
    "    sci_df['cleaned_text'] = sci_df['text'].apply(preprocess_tweet)\n",
    "\n",
    "# 4. Vectorisation TF-IDF\n",
    "vectorizer = TfidfVectorizer(ngram_range=(1, 2), max_features=20000)\n",
    "X = vectorizer.fit_transform(sci_df['cleaned_text'])\n",
    "\n",
    "# 5. Cr√©ation des cibles multi-labels\n",
    "y = sci_df[['is_claim_or_ref', 'is_context']].values\n",
    "\n",
    "# 6. Utilisation d'un mod√®le adapt√© au multi-label (ex: ClassifierChain + RandomForest)\n",
    "from sklearn.multioutput import ClassifierChain\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "\n",
    "model = ClassifierChain(\n",
    "    RandomForestClassifier(random_state=42),\n",
    "    order='random',\n",
    "    random_state=42\n",
    ")\n",
    "\n",
    "# 7. √âvaluation par validation crois√©e\n",
    "from sklearn.model_selection import cross_val_predict\n",
    "from sklearn.metrics import classification_report\n",
    "\n",
    "y_pred = cross_val_predict(model, X, y, cv=10)\n",
    "print(\"\\nRapport de classification pour chaque label :\")\n",
    "print(classification_report(y, y_pred, target_names=['CLAIM/REF', 'CONTEXT']))\n",
    "\n",
    "from sklearn.metrics import accuracy_score\n",
    "\n",
    "# Accuracy multi-label stricte (exact match ratio)\n",
    "exact_match_accuracy = accuracy_score(y, y_pred)\n",
    "print(f\"\\nAccuracy stricte (exact match) : {exact_match_accuracy:.4f}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cc586db6-6e42-4495-bef4-27a87bee6a9a",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "id": "42f28553-4858-4a2f-affd-c0923aad4293",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "                                                 text  is_claim_or_ref  \\\n",
      "2   Can any Gynecologist with Cancer Experience ex...                1   \n",
      "3   Couch-lock highs lead to sleeping in the couch...                1   \n",
      "4   Does daily routine help prevent problems with ...                1   \n",
      "6   ‚ÄúTraffic Jam‚Äù In Brain‚Äôs Neurons Could Be Caus...                1   \n",
      "7   Can playing more games improve lives and save ...                1   \n",
      "8   The effect of climate change on iceberg produc...                1   \n",
      "14  @RepCohen @SenAlexander @SenBobCorker pls supp...                0   \n",
      "15  Poverty is the greatest cause behind child lab...                1   \n",
      "16  \"@TheFactsBook: Drinking chocolate milk has be...                1   \n",
      "24  A wise Physician said, ‚ÄúThe best medicine for ...                1   \n",
      "\n",
      "    is_context  pred_claim_ref  pred_context  \n",
      "2            0             1.0           0.0  \n",
      "3            0             1.0           0.0  \n",
      "4            0             1.0           1.0  \n",
      "6            1             1.0           1.0  \n",
      "7            0             1.0           1.0  \n",
      "8            1             1.0           1.0  \n",
      "14           1             1.0           0.0  \n",
      "15           0             1.0           0.0  \n",
      "16           0             1.0           0.0  \n",
      "24           0             1.0           0.0  \n"
     ]
    }
   ],
   "source": [
    "#pip install imbalanced-learn\n",
    "# Exemple de tweets avec pr√©dictions\n",
    "sample_results = sci_df.copy()\n",
    "sample_results[['pred_claim_ref', 'pred_context']] = y_pred\n",
    "print(sample_results[['text', 'is_claim_or_ref', 'is_context', 'pred_claim_ref', 'pred_context']].head(10))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "id": "13f4e05e-2c81-46be-baa9-ccf1d2ece2a3",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Distribution des combinaisons :\n",
      "is_claim_or_ref  is_context\n",
      "1                1             218\n",
      "                 0             124\n",
      "0                1              33\n",
      "Name: count, dtype: int64\n",
      "\n",
      "============================================================\n",
      "Optimisation de SVM\n",
      "============================================================\n",
      "Fitting 10 folds for each of 6 candidates, totalling 60 fits\n",
      "\n",
      "Meilleurs param√®tres: {'estimator__C': 10, 'estimator__kernel': 'linear'}\n",
      "Accuracy (moyenne ¬± √©cart-type): 0.8094 ¬± 0.0427\n",
      "Scores par fold: [0.73684211 0.82894737 0.80263158 0.81578947 0.82894737 0.83783784\n",
      " 0.82432432 0.89189189 0.75675676 0.77027027]\n",
      "\n",
      "============================================================\n",
      "Optimisation de Naive Bayes\n",
      "============================================================\n",
      "Fitting 10 folds for each of 6 candidates, totalling 60 fits\n",
      "\n",
      "Meilleurs param√®tres: {'estimator__alpha': 0.5, 'estimator__fit_prior': True}\n",
      "Accuracy (moyenne ¬± √©cart-type): 0.8053 ¬± 0.0415\n",
      "Scores par fold: [0.75       0.81578947 0.81578947 0.78947368 0.86842105 0.7972973\n",
      " 0.81081081 0.87837838 0.78378378 0.74324324]\n",
      "\n",
      "============================================================\n",
      "Optimisation de k-NN\n",
      "============================================================\n",
      "Fitting 10 folds for each of 12 candidates, totalling 120 fits\n",
      "\n",
      "Meilleurs param√®tres: {'estimator__metric': 'euclidean', 'estimator__n_neighbors': 5, 'estimator__weights': 'distance'}\n",
      "Accuracy (moyenne ¬± √©cart-type): 0.8148 ¬± 0.0398\n",
      "Scores par fold: [0.78947368 0.78947368 0.81578947 0.77631579 0.85526316 0.78378378\n",
      " 0.89189189 0.86486486 0.77027027 0.81081081]\n",
      "\n",
      "============================================================\n",
      "Optimisation de Random Forest\n",
      "============================================================\n",
      "Fitting 10 folds for each of 12 candidates, totalling 120 fits\n",
      "\n",
      "Meilleurs param√®tres: {'estimator__max_depth': None, 'estimator__min_samples_split': 2, 'estimator__n_estimators': 100}\n",
      "Accuracy (moyenne ¬± √©cart-type): 0.8174 ¬± 0.0412\n",
      "Scores par fold: [0.76315789 0.82894737 0.81578947 0.81578947 0.82894737 0.85135135\n",
      " 0.82432432 0.90540541 0.75675676 0.78378378]\n",
      "\n",
      "============================================================\n",
      "Optimisation de Logistic Regression\n",
      "============================================================\n",
      "Fitting 10 folds for each of 6 candidates, totalling 60 fits\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/tiziri-tamani/anaconda3/lib/python3.12/site-packages/sklearn/svm/_base.py:1235: ConvergenceWarning: Liblinear failed to converge, increase the number of iterations.\n",
      "  warnings.warn(\n",
      "/home/tiziri-tamani/anaconda3/lib/python3.12/site-packages/sklearn/svm/_base.py:1235: ConvergenceWarning: Liblinear failed to converge, increase the number of iterations.\n",
      "  warnings.warn(\n",
      "/home/tiziri-tamani/anaconda3/lib/python3.12/site-packages/sklearn/svm/_base.py:1235: ConvergenceWarning: Liblinear failed to converge, increase the number of iterations.\n",
      "  warnings.warn(\n",
      "/home/tiziri-tamani/anaconda3/lib/python3.12/site-packages/sklearn/svm/_base.py:1235: ConvergenceWarning: Liblinear failed to converge, increase the number of iterations.\n",
      "  warnings.warn(\n",
      "/home/tiziri-tamani/anaconda3/lib/python3.12/site-packages/sklearn/svm/_base.py:1235: ConvergenceWarning: Liblinear failed to converge, increase the number of iterations.\n",
      "  warnings.warn(\n",
      "/home/tiziri-tamani/anaconda3/lib/python3.12/site-packages/sklearn/svm/_base.py:1235: ConvergenceWarning: Liblinear failed to converge, increase the number of iterations.\n",
      "  warnings.warn(\n",
      "/home/tiziri-tamani/anaconda3/lib/python3.12/site-packages/sklearn/svm/_base.py:1235: ConvergenceWarning: Liblinear failed to converge, increase the number of iterations.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Meilleurs param√®tres: {'estimator__C': 10, 'estimator__penalty': 'l1', 'estimator__solver': 'liblinear'}\n",
      "Accuracy (moyenne ¬± √©cart-type): 0.8627 ¬± 0.0330\n",
      "Scores par fold: [0.80263158 0.89473684 0.86842105 0.85526316 0.86842105 0.89189189\n",
      " 0.86486486 0.91891892 0.82432432 0.83783784]\n",
      "\n",
      "============================================================\n",
      "Optimisation de Gradient Boosting\n",
      "============================================================\n",
      "Fitting 10 folds for each of 8 candidates, totalling 80 fits\n",
      "\n",
      "Meilleurs param√®tres: {'estimator__learning_rate': 0.1, 'estimator__max_depth': 3, 'estimator__n_estimators': 50}\n",
      "Accuracy (moyenne ¬± √©cart-type): 0.8721 ¬± 0.0489\n",
      "Scores par fold: [0.80263158 0.90789474 0.88157895 0.85526316 0.88157895 0.93243243\n",
      " 0.82432432 0.95945946 0.81081081 0.86486486]\n"
     ]
    }
   ],
   "source": [
    "# ------------------------------------------------------------\n",
    "# √âTAPE 2 : Classification Binaire Multi-Label - Version Corrig√©e\n",
    "# ------------------------------------------------------------\n",
    "\n",
    "from sklearn.model_selection import KFold\n",
    "from sklearn.metrics import make_scorer\n",
    "import numpy as np\n",
    "from sklearn.ensemble import GradientBoostingClassifier\n",
    "from sklearn.model_selection import KFold\n",
    "from sklearn.metrics import make_scorer, f1_score\n",
    "from imblearn.ensemble import BalancedRandomForestClassifier\n",
    "from iterstrat.ml_stratifiers import MultilabelStratifiedKFold\n",
    "from sklearn.multioutput import MultiOutputClassifier\n",
    "\n",
    "\n",
    "stop_words = set(stopwords.words('english'))\n",
    "custom_stop_words = {\"http\", \"https\", \"rt\", \"co\", \"amp\", \"via\"}\n",
    "stop_words.update(custom_stop_words)\n",
    "lemmatizer = WordNetLemmatizer()\n",
    "\n",
    "def preprocess_tweet(tweet):\n",
    "    # 1. Supprime mentions (@) et caract√®res sp√©ciaux\n",
    "    #tweet = re.sub(r\"@\\w+|\\W\", \" \", tweet)\n",
    "    # 2. Convertit en minuscules\n",
    "    tweet = tweet.lower()\n",
    "    # 3. Tokenization et lemmatisation\n",
    "    tokens = word_tokenize(tweet)\n",
    "    tokens = [lemmatizer.lemmatize(word) for word in tokens if word not in stop_words]\n",
    "    # 4. Recombine en texte\n",
    "    return \" \".join(tokens)\n",
    "\n",
    "\n",
    "# 1. Pr√©paration des donn√©es (tweets scientifiques uniquement)\n",
    "sci_df = df[df['science_related'] == 1].copy()\n",
    "\n",
    "# 2. Cr√©ation des DEUX colonnes cibles (peuvent valoir 1 simultan√©ment)\n",
    "sci_df['is_claim_or_ref'] = ((sci_df['scientific_claim'] == 1) | (sci_df['scientific_reference'] == 1)).astype(int)\n",
    "sci_df['is_context'] = (sci_df['scientific_context'] == 1).astype(int)\n",
    "\n",
    "# Afficher les combinaisons possibles\n",
    "print(\"Distribution des combinaisons :\")\n",
    "print(sci_df[['is_claim_or_ref', 'is_context']].value_counts())\n",
    "\n",
    "# 3. Pr√©traitement du texte (si pas d√©j√† fait)\n",
    "if 'cleaned_text' not in sci_df.columns:\n",
    "    sci_df['cleaned_text'] = sci_df['text'].apply(preprocess_tweet)\n",
    "\n",
    "# 4. Vectorisation TF-IDF\n",
    "vectorizer = TfidfVectorizer(ngram_range=(1, 3), max_features=20000)\n",
    "X = vectorizer.fit_transform(sci_df['cleaned_text'])\n",
    "\n",
    "# 5. Cr√©ation des cibles multi-labels\n",
    "y = sci_df[['is_claim_or_ref', 'is_context']].values\n",
    "\n",
    "\n",
    "# Fonction de scoring personnalis√©e pour multi-label\n",
    "def multilabel_accuracy(y_true, y_pred):\n",
    "    return np.mean(y_true == y_pred)\n",
    "\n",
    "# Configuration KFold\n",
    "kf = KFold(n_splits=10, shuffle=True, random_state=42)\n",
    "scorer = make_scorer(multilabel_accuracy)\n",
    "\n",
    "# Dictionnaire des mod√®les r√©vis√©\n",
    "models = {\n",
    "    \"SVM\": {\n",
    "        \"model\": SVC(probability=True),\n",
    "        \"params\": {\n",
    "            'estimator__C': [0.1, 1, 10],\n",
    "            'estimator__kernel': ['linear', 'rbf']\n",
    "        }\n",
    "    },\n",
    "    \"Naive Bayes\": {\n",
    "        \"model\": MultinomialNB(),\n",
    "        \"params\": {\n",
    "            'estimator__alpha': [0.1, 0.5, 1.0],\n",
    "            'estimator__fit_prior': [True, False]\n",
    "        }\n",
    "    },\n",
    "    \"k-NN\": {\n",
    "        \"model\": KNeighborsClassifier(),\n",
    "        \"params\": {\n",
    "            'estimator__n_neighbors': [3, 5, 7],\n",
    "            'estimator__weights': ['uniform', 'distance'],\n",
    "            'estimator__metric': ['euclidean', 'cosine']\n",
    "        }\n",
    "    },\n",
    "    \"Random Forest\": {\n",
    "        \"model\": RandomForestClassifier(random_state=42),\n",
    "        \"params\": {\n",
    "            'estimator__n_estimators': [50, 100],\n",
    "            'estimator__max_depth': [None, 10, 20],\n",
    "            'estimator__min_samples_split': [2, 5]\n",
    "        }\n",
    "    },\n",
    "    \"Logistic Regression\": {\n",
    "        \"model\": LogisticRegression(max_iter=1000),\n",
    "        \"params\": {\n",
    "            'estimator__C': [0.1, 1, 10],\n",
    "            'estimator__penalty': ['l1', 'l2'],\n",
    "            'estimator__solver': ['liblinear']\n",
    "        }\n",
    "    },\n",
    "    \"Gradient Boosting\": {\n",
    "        \"model\": GradientBoostingClassifier(random_state=42),\n",
    "        \"params\": {\n",
    "            'estimator__n_estimators': [50, 100],\n",
    "            'estimator__learning_rate': [0.01, 0.1],\n",
    "            'estimator__max_depth': [3, 5]\n",
    "        }\n",
    "    }\n",
    "}\n",
    "\n",
    "# √âvaluation des mod√®les\n",
    "for name, config in models.items():\n",
    "    print(f\"\\n{'='*60}\")\n",
    "    print(f\"Optimisation de {name}\")\n",
    "    print(f\"{'='*60}\")\n",
    "    \n",
    "    # Cr√©ation du pipeline\n",
    "    mo = MultiOutputClassifier(config['model'])\n",
    "    \n",
    "    # GridSearch avec validation crois√©e interne\n",
    "    grid = GridSearchCV(\n",
    "        estimator=mo,\n",
    "        param_grid=config['params'],\n",
    "        cv=10,\n",
    "        scoring=scorer,\n",
    "        n_jobs=-1,\n",
    "        verbose=1\n",
    "    )\n",
    "    \n",
    "    # IMPORTANT: On doit fit le GridSearch avant d'acc√©der aux best_params_\n",
    "    grid.fit(X, y)  # <-- Cette ligne √©tait manquante\n",
    "    \n",
    "    # Cross-validation externe\n",
    "    cv_scores = cross_val_score(\n",
    "        grid.best_estimator_,\n",
    "        X,\n",
    "        y,\n",
    "        cv=kf,\n",
    "        scoring=scorer,\n",
    "        n_jobs=-1\n",
    "    )\n",
    "    \n",
    "    print(f\"\\nMeilleurs param√®tres: {grid.best_params_}\")\n",
    "    print(f\"Accuracy (moyenne ¬± √©cart-type): {cv_scores.mean():.4f} ¬± {cv_scores.std():.4f}\")\n",
    "    print(f\"Scores par fold: {cv_scores}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3e7aef34-b823-49c9-ba22-5cfd9ae0a927",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "id": "3743ae31-a2ac-4559-99ef-4c7f3e61b691",
   "metadata": {},
   "outputs": [
    {
     "ename": "KeyError",
     "evalue": "'is_claim_or_ref'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyError\u001b[0m                                  Traceback (most recent call last)",
      "File \u001b[0;32m~/anaconda3/lib/python3.12/site-packages/pandas/core/indexes/base.py:3805\u001b[0m, in \u001b[0;36mIndex.get_loc\u001b[0;34m(self, key)\u001b[0m\n\u001b[1;32m   3804\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[0;32m-> 3805\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_engine\u001b[38;5;241m.\u001b[39mget_loc(casted_key)\n\u001b[1;32m   3806\u001b[0m \u001b[38;5;28;01mexcept\u001b[39;00m \u001b[38;5;167;01mKeyError\u001b[39;00m \u001b[38;5;28;01mas\u001b[39;00m err:\n",
      "File \u001b[0;32mindex.pyx:167\u001b[0m, in \u001b[0;36mpandas._libs.index.IndexEngine.get_loc\u001b[0;34m()\u001b[0m\n",
      "File \u001b[0;32mindex.pyx:196\u001b[0m, in \u001b[0;36mpandas._libs.index.IndexEngine.get_loc\u001b[0;34m()\u001b[0m\n",
      "File \u001b[0;32mpandas/_libs/hashtable_class_helper.pxi:7081\u001b[0m, in \u001b[0;36mpandas._libs.hashtable.PyObjectHashTable.get_item\u001b[0;34m()\u001b[0m\n",
      "File \u001b[0;32mpandas/_libs/hashtable_class_helper.pxi:7089\u001b[0m, in \u001b[0;36mpandas._libs.hashtable.PyObjectHashTable.get_item\u001b[0;34m()\u001b[0m\n",
      "\u001b[0;31mKeyError\u001b[0m: 'is_claim_or_ref'",
      "\nThe above exception was the direct cause of the following exception:\n",
      "\u001b[0;31mKeyError\u001b[0m                                  Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[30], line 30\u001b[0m\n\u001b[1;32m     27\u001b[0m sci_df[\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mcleaned_text\u001b[39m\u001b[38;5;124m'\u001b[39m] \u001b[38;5;241m=\u001b[39m sci_df[\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mtext\u001b[39m\u001b[38;5;124m'\u001b[39m]\u001b[38;5;241m.\u001b[39mapply(preprocess_tweet)\n\u001b[1;32m     29\u001b[0m \u001b[38;5;66;03m# Cr√©ation des labels combin√©s\u001b[39;00m\n\u001b[0;32m---> 30\u001b[0m sci_df[\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mlabels\u001b[39m\u001b[38;5;124m'\u001b[39m] \u001b[38;5;241m=\u001b[39m sci_df\u001b[38;5;241m.\u001b[39mapply(\u001b[38;5;28;01mlambda\u001b[39;00m row: \n\u001b[1;32m     31\u001b[0m                                [\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mclaim_or_ref\u001b[39m\u001b[38;5;124m'\u001b[39m] \u001b[38;5;28;01mif\u001b[39;00m row[\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mis_claim_or_ref\u001b[39m\u001b[38;5;124m'\u001b[39m] \u001b[38;5;28;01melse\u001b[39;00m [] \u001b[38;5;241m+\u001b[39m \n\u001b[1;32m     32\u001b[0m                                [\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mcontext\u001b[39m\u001b[38;5;124m'\u001b[39m] \u001b[38;5;28;01mif\u001b[39;00m row[\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mis_context\u001b[39m\u001b[38;5;124m'\u001b[39m] \u001b[38;5;28;01melse\u001b[39;00m [], axis\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m1\u001b[39m)\n\u001b[1;32m     34\u001b[0m \u001b[38;5;66;03m# Transformation multi-label\u001b[39;00m\n\u001b[1;32m     35\u001b[0m mlb \u001b[38;5;241m=\u001b[39m MultiLabelBinarizer()\n",
      "File \u001b[0;32m~/anaconda3/lib/python3.12/site-packages/pandas/core/frame.py:10374\u001b[0m, in \u001b[0;36mDataFrame.apply\u001b[0;34m(self, func, axis, raw, result_type, args, by_row, engine, engine_kwargs, **kwargs)\u001b[0m\n\u001b[1;32m  10360\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01mpandas\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mcore\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mapply\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m frame_apply\n\u001b[1;32m  10362\u001b[0m op \u001b[38;5;241m=\u001b[39m frame_apply(\n\u001b[1;32m  10363\u001b[0m     \u001b[38;5;28mself\u001b[39m,\n\u001b[1;32m  10364\u001b[0m     func\u001b[38;5;241m=\u001b[39mfunc,\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m  10372\u001b[0m     kwargs\u001b[38;5;241m=\u001b[39mkwargs,\n\u001b[1;32m  10373\u001b[0m )\n\u001b[0;32m> 10374\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m op\u001b[38;5;241m.\u001b[39mapply()\u001b[38;5;241m.\u001b[39m__finalize__(\u001b[38;5;28mself\u001b[39m, method\u001b[38;5;241m=\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mapply\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n",
      "File \u001b[0;32m~/anaconda3/lib/python3.12/site-packages/pandas/core/apply.py:916\u001b[0m, in \u001b[0;36mFrameApply.apply\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    913\u001b[0m \u001b[38;5;28;01melif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mraw:\n\u001b[1;32m    914\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mapply_raw(engine\u001b[38;5;241m=\u001b[39m\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mengine, engine_kwargs\u001b[38;5;241m=\u001b[39m\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mengine_kwargs)\n\u001b[0;32m--> 916\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mapply_standard()\n",
      "File \u001b[0;32m~/anaconda3/lib/python3.12/site-packages/pandas/core/apply.py:1063\u001b[0m, in \u001b[0;36mFrameApply.apply_standard\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m   1061\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mapply_standard\u001b[39m(\u001b[38;5;28mself\u001b[39m):\n\u001b[1;32m   1062\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mengine \u001b[38;5;241m==\u001b[39m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mpython\u001b[39m\u001b[38;5;124m\"\u001b[39m:\n\u001b[0;32m-> 1063\u001b[0m         results, res_index \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mapply_series_generator()\n\u001b[1;32m   1064\u001b[0m     \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m   1065\u001b[0m         results, res_index \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mapply_series_numba()\n",
      "File \u001b[0;32m~/anaconda3/lib/python3.12/site-packages/pandas/core/apply.py:1081\u001b[0m, in \u001b[0;36mFrameApply.apply_series_generator\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m   1078\u001b[0m \u001b[38;5;28;01mwith\u001b[39;00m option_context(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mmode.chained_assignment\u001b[39m\u001b[38;5;124m\"\u001b[39m, \u001b[38;5;28;01mNone\u001b[39;00m):\n\u001b[1;32m   1079\u001b[0m     \u001b[38;5;28;01mfor\u001b[39;00m i, v \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28menumerate\u001b[39m(series_gen):\n\u001b[1;32m   1080\u001b[0m         \u001b[38;5;66;03m# ignore SettingWithCopy here in case the user mutates\u001b[39;00m\n\u001b[0;32m-> 1081\u001b[0m         results[i] \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mfunc(v, \u001b[38;5;241m*\u001b[39m\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39m\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mkwargs)\n\u001b[1;32m   1082\u001b[0m         \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(results[i], ABCSeries):\n\u001b[1;32m   1083\u001b[0m             \u001b[38;5;66;03m# If we have a view on v, we need to make a copy because\u001b[39;00m\n\u001b[1;32m   1084\u001b[0m             \u001b[38;5;66;03m#  series_generator will swap out the underlying data\u001b[39;00m\n\u001b[1;32m   1085\u001b[0m             results[i] \u001b[38;5;241m=\u001b[39m results[i]\u001b[38;5;241m.\u001b[39mcopy(deep\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mFalse\u001b[39;00m)\n",
      "Cell \u001b[0;32mIn[30], line 31\u001b[0m, in \u001b[0;36m<lambda>\u001b[0;34m(row)\u001b[0m\n\u001b[1;32m     27\u001b[0m sci_df[\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mcleaned_text\u001b[39m\u001b[38;5;124m'\u001b[39m] \u001b[38;5;241m=\u001b[39m sci_df[\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mtext\u001b[39m\u001b[38;5;124m'\u001b[39m]\u001b[38;5;241m.\u001b[39mapply(preprocess_tweet)\n\u001b[1;32m     29\u001b[0m \u001b[38;5;66;03m# Cr√©ation des labels combin√©s\u001b[39;00m\n\u001b[1;32m     30\u001b[0m sci_df[\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mlabels\u001b[39m\u001b[38;5;124m'\u001b[39m] \u001b[38;5;241m=\u001b[39m sci_df\u001b[38;5;241m.\u001b[39mapply(\u001b[38;5;28;01mlambda\u001b[39;00m row: \n\u001b[0;32m---> 31\u001b[0m                                [\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mclaim_or_ref\u001b[39m\u001b[38;5;124m'\u001b[39m] \u001b[38;5;28;01mif\u001b[39;00m row[\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mis_claim_or_ref\u001b[39m\u001b[38;5;124m'\u001b[39m] \u001b[38;5;28;01melse\u001b[39;00m [] \u001b[38;5;241m+\u001b[39m \n\u001b[1;32m     32\u001b[0m                                [\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mcontext\u001b[39m\u001b[38;5;124m'\u001b[39m] \u001b[38;5;28;01mif\u001b[39;00m row[\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mis_context\u001b[39m\u001b[38;5;124m'\u001b[39m] \u001b[38;5;28;01melse\u001b[39;00m [], axis\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m1\u001b[39m)\n\u001b[1;32m     34\u001b[0m \u001b[38;5;66;03m# Transformation multi-label\u001b[39;00m\n\u001b[1;32m     35\u001b[0m mlb \u001b[38;5;241m=\u001b[39m MultiLabelBinarizer()\n",
      "File \u001b[0;32m~/anaconda3/lib/python3.12/site-packages/pandas/core/series.py:1121\u001b[0m, in \u001b[0;36mSeries.__getitem__\u001b[0;34m(self, key)\u001b[0m\n\u001b[1;32m   1118\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_values[key]\n\u001b[1;32m   1120\u001b[0m \u001b[38;5;28;01melif\u001b[39;00m key_is_scalar:\n\u001b[0;32m-> 1121\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_get_value(key)\n\u001b[1;32m   1123\u001b[0m \u001b[38;5;66;03m# Convert generator to list before going through hashable part\u001b[39;00m\n\u001b[1;32m   1124\u001b[0m \u001b[38;5;66;03m# (We will iterate through the generator there to check for slices)\u001b[39;00m\n\u001b[1;32m   1125\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m is_iterator(key):\n",
      "File \u001b[0;32m~/anaconda3/lib/python3.12/site-packages/pandas/core/series.py:1237\u001b[0m, in \u001b[0;36mSeries._get_value\u001b[0;34m(self, label, takeable)\u001b[0m\n\u001b[1;32m   1234\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_values[label]\n\u001b[1;32m   1236\u001b[0m \u001b[38;5;66;03m# Similar to Index.get_value, but we do not fall back to positional\u001b[39;00m\n\u001b[0;32m-> 1237\u001b[0m loc \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mindex\u001b[38;5;241m.\u001b[39mget_loc(label)\n\u001b[1;32m   1239\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m is_integer(loc):\n\u001b[1;32m   1240\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_values[loc]\n",
      "File \u001b[0;32m~/anaconda3/lib/python3.12/site-packages/pandas/core/indexes/base.py:3812\u001b[0m, in \u001b[0;36mIndex.get_loc\u001b[0;34m(self, key)\u001b[0m\n\u001b[1;32m   3807\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(casted_key, \u001b[38;5;28mslice\u001b[39m) \u001b[38;5;129;01mor\u001b[39;00m (\n\u001b[1;32m   3808\u001b[0m         \u001b[38;5;28misinstance\u001b[39m(casted_key, abc\u001b[38;5;241m.\u001b[39mIterable)\n\u001b[1;32m   3809\u001b[0m         \u001b[38;5;129;01mand\u001b[39;00m \u001b[38;5;28many\u001b[39m(\u001b[38;5;28misinstance\u001b[39m(x, \u001b[38;5;28mslice\u001b[39m) \u001b[38;5;28;01mfor\u001b[39;00m x \u001b[38;5;129;01min\u001b[39;00m casted_key)\n\u001b[1;32m   3810\u001b[0m     ):\n\u001b[1;32m   3811\u001b[0m         \u001b[38;5;28;01mraise\u001b[39;00m InvalidIndexError(key)\n\u001b[0;32m-> 3812\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mKeyError\u001b[39;00m(key) \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01merr\u001b[39;00m\n\u001b[1;32m   3813\u001b[0m \u001b[38;5;28;01mexcept\u001b[39;00m \u001b[38;5;167;01mTypeError\u001b[39;00m:\n\u001b[1;32m   3814\u001b[0m     \u001b[38;5;66;03m# If we have a listlike key, _check_indexing_error will raise\u001b[39;00m\n\u001b[1;32m   3815\u001b[0m     \u001b[38;5;66;03m#  InvalidIndexError. Otherwise we fall through and re-raise\u001b[39;00m\n\u001b[1;32m   3816\u001b[0m     \u001b[38;5;66;03m#  the TypeError.\u001b[39;00m\n\u001b[1;32m   3817\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_check_indexing_error(key)\n",
      "\u001b[0;31mKeyError\u001b[0m: 'is_claim_or_ref'"
     ]
    }
   ],
   "source": [
    "# ------------------------------------------------------------\n",
    "# √âTAPE 2 : Classification Multi-Label avec √âquilibrage Adapt√©\n",
    "# ------------------------------------------------------------\n",
    "\n",
    "import numpy as np\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "from sklearn.model_selection import train_test_split, GridSearchCV\n",
    "from sklearn.multioutput import MultiOutputClassifier\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from sklearn.metrics import classification_report, f1_score\n",
    "from sklearn.preprocessing import MultiLabelBinarizer\n",
    "\n",
    "## 1. Pr√©traitement des donn√©es (identique √† votre version)\n",
    "stop_words = set(stopwords.words('english'))\n",
    "custom_stop_words = {\"http\", \"https\", \"rt\", \"co\", \"amp\", \"via\"}\n",
    "stop_words.update(custom_stop_words)\n",
    "lemmatizer = WordNetLemmatizer()\n",
    "\n",
    "def preprocess_tweet(tweet):\n",
    "    tweet = tweet.lower()\n",
    "    tokens = word_tokenize(tweet)\n",
    "    tokens = [lemmatizer.lemmatize(word) for word in tokens if word not in stop_words]\n",
    "    return \" \".join(tokens)\n",
    "\n",
    "## 2. Pr√©paration des donn√©es\n",
    "sci_df = df[df['science_related'] == 1].copy()\n",
    "sci_df['cleaned_text'] = sci_df['text'].apply(preprocess_tweet)\n",
    "\n",
    "# Cr√©ation des labels combin√©s\n",
    "sci_df['labels'] = sci_df.apply(lambda row: \n",
    "                               ['claim_or_ref'] if row['is_claim_or_ref'] else [] + \n",
    "                               ['context'] if row['is_context'] else [], axis=1)\n",
    "\n",
    "# Transformation multi-label\n",
    "mlb = MultiLabelBinarizer()\n",
    "y = mlb.fit_transform(sci_df['labels'])\n",
    "X = vectorizer.fit_transform(sci_df['cleaned_text'])\n",
    "\n",
    "## 3. √âquilibrage Adapt√© (Solution pour contourner la limitation d'imblearn)\n",
    "def balanced_train_test_split(X, y, test_size=0.2):\n",
    "    # S√©paration stratifi√©e\n",
    "    X_train, X_test, y_train, y_test = train_test_split(\n",
    "        X, y, test_size=test_size, stratify=y.sum(axis=1), random_state=42\n",
    "    )\n",
    "    \n",
    "    # R√©√©chantillonnage manuel pour l'entra√Ænement\n",
    "    label_counts = y_train.sum(axis=0)\n",
    "    max_count = max(label_counts)\n",
    "    \n",
    "    resampled_indices = []\n",
    "    for label_idx in range(y.shape[1]):\n",
    "        label_indices = np.where(y_train[:, label_idx] == 1)[0]\n",
    "        resampled_indices.extend(\n",
    "            np.random.choice(label_indices, size=max_count, replace=True))\n",
    "        \n",
    "    X_train_res = X_train[resampled_indices]\n",
    "    y_train_res = y_train[resampled_indices]\n",
    "    \n",
    "    return X_train_res, X_test, y_train_res, y_test\n",
    "\n",
    "X_train, X_test, y_train, y_test = balanced_train_test_split(X, y)\n",
    "\n",
    "## 4. Mod√®le et √âvaluation\n",
    "model = MultiOutputClassifier(\n",
    "    RandomForestClassifier(\n",
    "        n_estimators=200,\n",
    "        max_depth=15,\n",
    "        class_weight='balanced',\n",
    "        random_state=42\n",
    "    )\n",
    ")\n",
    "\n",
    "model.fit(X_train, y_train)\n",
    "y_pred = model.predict(X_test)\n",
    "\n",
    "print(\"Rapport de Classification:\")\n",
    "print(classification_report(y_test, y_pred, target_names=mlb.classes_))\n",
    "\n",
    "## 5. Optimisation avec GridSearchCV\n",
    "param_grid = {\n",
    "    'estimator__n_estimators': [100, 200],\n",
    "    'estimator__max_depth': [10, 15, None],\n",
    "    'estimator__min_samples_split': [2, 5]\n",
    "}\n",
    "\n",
    "grid = GridSearchCV(\n",
    "    model, \n",
    "    param_grid, \n",
    "    cv=5,\n",
    "    scoring='f1_micro',\n",
    "    n_jobs=-1,\n",
    "    verbose=1\n",
    ")\n",
    "grid.fit(X_train, y_train)\n",
    "\n",
    "print(f\"\\nMeilleurs param√®tres: {grid.best_params_}\")\n",
    "best_model = grid.best_estimator_\n",
    "y_pred = best_model.predict(X_test)\n",
    "print(\"\\nPerformance du meilleur mod√®le:\")\n",
    "print(classification_report(y_test, y_pred, target_names=mlb.classes_))"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
