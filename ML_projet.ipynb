{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "be9258fe",
   "metadata": {},
   "source": [
    "# ML projet - Groupe 3"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "33b691a8",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "176f2932",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "5c7e69b8",
   "metadata": {},
   "source": [
    "# Pretraitement"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 100,
   "id": "7e670a9b",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "from gensim.models import Word2Vec\n",
    "import nltk\n",
    "import re\n",
    "from nltk import word_tokenize, pos_tag\n",
    "from nltk.corpus import words\n",
    "from nltk.stem import WordNetLemmatizer\n",
    "from nltk.corpus import wordnet\n",
    "import enchant\n",
    "import numpy as np\n",
    "from collections import Counter\n",
    "import seaborn as sns\n",
    "\n",
    "\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "from sklearn.model_selection import train_test_split\n",
    "from imblearn.over_sampling import SMOTE\n",
    "import random\n",
    "from sklearn.model_selection import KFold\n",
    "\n",
    "\n",
    "\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from sklearn.svm import SVC\n",
    "from sklearn.metrics import classification_report, accuracy_score\n",
    "from sklearn.model_selection import cross_val_score\n",
    "\n",
    "from sklearn.decomposition import PCA\n",
    "import matplotlib.pyplot as plt\n",
    "from sklearn.manifold import TSNE\n",
    "\n",
    "import emoji\n",
    "from scipy.sparse import hstack\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from nltk.corpus import stopwords\n",
    "\n",
    "\n",
    "\n",
    "#nltk.download('punkt_tab')\n",
    "#nltk.download(\"words\")\n",
    "#nltk.download('wordnet')\n",
    "#nltk.download('averaged_perceptron_tagger_eng')\n",
    "#%matplotlib qt\n",
    "#nltk.download('stopwords')\n",
    "%matplotlib inline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 101,
   "id": "8b05a6f0",
   "metadata": {},
   "outputs": [],
   "source": [
    "df = pd.read_csv(\"scitweets_export.tsv\", sep = \"\\t\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 102,
   "id": "01c83bfe",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.6710526315789473"
      ]
     },
     "execution_count": 102,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(df[df[\"science_related\"] == 0])/(len(df))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "706f152b",
   "metadata": {},
   "source": [
    "### Enlever (.,\\\\@#:';\"“”’[]), les chiffres et les liens (http) de chaque tweet"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 103,
   "id": "b1c68b92",
   "metadata": {},
   "outputs": [],
   "source": [
    "text = []\n",
    "all_text = list(df[\"text\"])\n",
    "for t in all_text:\n",
    "    a = re.sub(\"[\\.,/\\\\@:;\\\"\\'0-9“”’\\[\\]]\", \"\", t)\n",
    "    #a = re.sub(\"http\\S*\", \"\", a) #Enlever les liens\n",
    "    a = re.sub(\"#\\S*\", \"\", a) #Enlever les tags\n",
    "    a = a.lower()\n",
    "    text.append(a)\n",
    "#for sentence in text:  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 104,
   "id": "d983e1ba",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Créer une liste des phrases ou chaque phrase est une liste de mots.\n",
    "text_sep = [word_tokenize(t) for t in text]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 105,
   "id": "9bb06303",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['whats', 'the', 'uber', 'support', 'team', 'email', 'address', '?']"
      ]
     },
     "execution_count": 105,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#len(text_sep)\n",
    "text_sep[-3]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d9c733d5",
   "metadata": {},
   "source": [
    "## Standardizer chaque phrases. \n",
    "- Convertir les noms pluriels en leur forme singuliers\n",
    "- Convertir les conjugaison de chaque verbe en leur infinitif\n",
    "- Standardiser les ajectives et les adverbes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 106,
   "id": "1dff66df",
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_wordnet_pos(tag):\n",
    "    if tag.startswith('J'):  # Adjectif\n",
    "        return wordnet.ADJ\n",
    "    elif tag.startswith('V'):  # Verbe\n",
    "        return wordnet.VERB\n",
    "    elif tag.startswith('N'):  # Nom\n",
    "        return wordnet.NOUN\n",
    "    elif tag.startswith('R'):  # Adverbe\n",
    "        return wordnet.ADV\n",
    "    else:\n",
    "        return wordnet.NOUN \n",
    "\n",
    "    \n",
    "lemmatizer = WordNetLemmatizer()\n",
    "refined_tweets = []\n",
    "for tweet in text_sep:\n",
    "    pos_tags = pos_tag(tweet)\n",
    "    lemmatized_words = [lemmatizer.lemmatize(word, get_wordnet_pos(tag)) for word, tag in pos_tags]\n",
    "    refined_tweets.append(lemmatized_words)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 111,
   "id": "5736e0ef",
   "metadata": {},
   "outputs": [],
   "source": [
    "tfidf = TfidfVectorizer(max_features=100000)  # tu choisis max_features selon ton cas\n",
    "X_text = tfidf.fit_transform(remake_sentences((refined_tweets)))\n",
    "y = df[\"science_related\"]\n",
    "\n",
    "kf = KFold(n_splits=10, shuffle=True, random_state=42)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 112,
   "id": "06606d8d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Scores par fold (logisitic regression): [0.78947368 0.77192982 0.76315789 0.75438596 0.85087719 0.74561404\n",
      " 0.86842105 0.81578947 0.81578947 0.85087719]\n",
      "Score moyen: 0.8026315789473684\n"
     ]
    }
   ],
   "source": [
    "logreg = LogisticRegression(class_weight='balanced', max_iter=1000)\n",
    "\n",
    "# Faire 5-fold cross-validation\n",
    "scores_rf = cross_val_score(logreg, X_text, y, cv=kf, scoring='accuracy')\n",
    "\n",
    "print(\"Scores par fold (logisitic regression):\", scores_rf)\n",
    "print(\"Score moyen:\", scores_rf.mean())"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8f9a5a6b",
   "metadata": {},
   "source": [
    "## Laisser une trace pour les emojis"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 109,
   "id": "5107bc83",
   "metadata": {},
   "outputs": [],
   "source": [
    "emoji_list = []\n",
    "has_emoji = False \n",
    "for sentence in refined_tweets:\n",
    "    has_emoji = False\n",
    "    for word in sentence:\n",
    "        for char in word:\n",
    "            if emoji.is_emoji(char):\n",
    "                has_emoji = True\n",
    "                break\n",
    "    if has_emoji is True:\n",
    "        emoji_list.append(1)\n",
    "    else:\n",
    "        emoji_list.append(0)\n",
    "df[\"has_emoji\"] = emoji_list"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 68,
   "id": "34a296f3",
   "metadata": {},
   "outputs": [],
   "source": [
    "def remake_sentences(sentence_list):\n",
    "    sentence_join = []\n",
    "    phrase = \"\"\n",
    "    for sentence in sentence_list:\n",
    "        for word in sentence:\n",
    "            phrase = phrase + word + \" \"\n",
    "        sentence_join.append(phrase)\n",
    "        phrase = \"\"\n",
    "    return(sentence_join)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 69,
   "id": "305adc88",
   "metadata": {},
   "outputs": [],
   "source": [
    "def plot_word_distribution(word_list):\n",
    "    word_list_flat = [word for sublist in word_list for word in sublist]\n",
    "    word_counts = Counter(word_list_flat)\n",
    "    # 3. Afficher les 10 mots les plus fréquents (optionnel)\n",
    "    most_common = word_counts.most_common(100)\n",
    "    #print(most_common[0:10])\n",
    "    words, counts = zip(*most_common)\n",
    "    \n",
    "    # 4. Tracer avec seaborn\n",
    "    #plt.figure(figsize=(30, 12))\n",
    "    plt.figure(figsize=(16, 8)).gca()\n",
    "    sns.barplot(x=list(words), y=list(counts))\n",
    "    plt.title(\"Distribution des mots\")\n",
    "    plt.ylabel(\"Nombre d'occurrences\")\n",
    "    plt.xlabel(\"Mots\")\n",
    "    plt.xticks(rotation=45)\n",
    "    plt.tight_layout()\n",
    "    plt.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "id": "fc49ab27",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/cm/.local/lib/python3.8/site-packages/matplotlib/backends/backend_agg.py:238: RuntimeWarning: Glyph 1 missing from current font.\n",
      "  font.set_text(s, 0.0, flags=flags)\n"
     ]
    }
   ],
   "source": [
    "plot_word_distribution(refined_tweets)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "id": "0584e171",
   "metadata": {},
   "outputs": [],
   "source": [
    "mots_a_supprimer = [\"the\", \"to\", \"a\", \"of\", \"and\", \"in\", \"for\", \"on\", \"this\", \n",
    "                    \"with\", \"that\", \"have\", \"by\", \"it\", \"from\", \"be\", \"do\", \n",
    "                    \"t\", \" \", \"to\", \"\\x01\", \"co\", \"no\", \"out\", \"d\", \"our\", \"so\",\n",
    "                   \"(\", \")\", \"your\", \"but\", \"if\", \"than\", \"\\x01\"]\n",
    "\n",
    "# Supprimer toutes les occurrences de \"the\"\n",
    "filtered_words = [[word for word in row if word not in mots_a_supprimer] for row in refined_tweets]\n",
    "#filtered_words = re.sub(r\"[\\x00-\\x1F]\", filtered_words)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "id": "05309462",
   "metadata": {},
   "outputs": [],
   "source": [
    "plot_word_distribution(filtered_words)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "35695a3a",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "id": "b6ec2b36",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/cm/.local/lib/python3.8/site-packages/matplotlib/backends/backend_agg.py:238: RuntimeWarning: Glyph 1 missing from current font.\n",
      "  font.set_text(s, 0.0, flags=flags)\n",
      "/home/cm/.local/lib/python3.8/site-packages/matplotlib/backends/backend_agg.py:201: RuntimeWarning: Glyph 1 missing from current font.\n",
      "  font.set_text(s, 0, flags=flags)\n"
     ]
    }
   ],
   "source": [
    "X = remake_sentences(filtered_words)  # Utiliser uniquement la colonne 'text' comme feature\n",
    "y_binary = df['science_related']  # Étiquette binaire (scientifique ou non)\n",
    "\n",
    "\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y_binary, test_size=0.2, random_state=1)\n",
    "\n",
    "vectorizer = TfidfVectorizer(ngram_range=(1, 2), max_features=100000)\n",
    "X_vec = vectorizer.fit_transform(X)  # Apprentissage et transformation sur l'entraînement\n",
    "\n",
    "X_train_vec = vectorizer.fit_transform(X_train)  # Apprentissage et transformation sur l'entraînement\n",
    "\n",
    "\n",
    "X_test_vec = vectorizer.transform(X_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "id": "e7e0dd1a",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "2128"
      ]
     },
     "execution_count": 58,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(X_test_vec.data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "id": "73e99764",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[139   7]\n",
      " [ 53  29]]\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.72      0.95      0.82       146\n",
      "           1       0.81      0.35      0.49        82\n",
      "\n",
      "    accuracy                           0.74       228\n",
      "   macro avg       0.76      0.65      0.66       228\n",
      "weighted avg       0.75      0.74      0.70       228\n",
      "\n"
     ]
    }
   ],
   "source": [
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from sklearn.pipeline import Pipeline\n",
    "from sklearn.metrics import classification_report, confusion_matrix\n",
    "from imblearn.over_sampling import SMOTE\n",
    "from imblearn.pipeline import Pipeline as ImbPipeline\n",
    "\n",
    "\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y_binary, test_size=0.2, random_state=42)\n",
    "\n",
    "# Création du pipeline\n",
    "pipeline = ImbPipeline([\n",
    "    ('vectorizer', TfidfVectorizer()),\n",
    "    ('smote', SMOTE(random_state=42)),\n",
    "    ('classifier', RandomForestClassifier(class_weight='balanced', random_state=42))\n",
    "])\n",
    "\n",
    "# Entraînement\n",
    "pipeline.fit(X_train, y_train)\n",
    "\n",
    "# Prédiction\n",
    "y_pred = pipeline.predict(X_test)\n",
    "\n",
    "# Évaluation\n",
    "print(confusion_matrix(y_test, y_pred))\n",
    "print(classification_report(y_test, y_pred))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "id": "f4bb4b06",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "F1 scores: [0.31914894 0.60504202 0.4957265  0.70072993 0.63865546]\n",
      "Moyenne: 0.5518605675791208\n"
     ]
    }
   ],
   "source": [
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from sklearn.pipeline import Pipeline\n",
    "from sklearn.metrics import classification_report, confusion_matrix\n",
    "from imblearn.over_sampling import SMOTE\n",
    "from imblearn.pipeline import Pipeline as ImbPipeline\n",
    "\n",
    "\n",
    "#X_train, X_test, y_train, y_test = train_test_split(list(df[\"text\"]), list(df[\"science_related\"]), test_size=0.2, random_state=42)\n",
    "\n",
    "# Création du pipeline\n",
    "pipeline = ImbPipeline([\n",
    "    ('vectorizer', TfidfVectorizer()),\n",
    "    ('smote', SMOTE(random_state=42)),\n",
    "    ('classifier', RandomForestClassifier(class_weight='balanced', random_state=42))\n",
    "])\n",
    "\n",
    "scores = cross_val_score(pipeline, list(df[\"text\"]), list(df[\"science_related\"]), cv=5, scoring='f1')\n",
    "print(\"F1 scores:\", scores)\n",
    "print(\"Moyenne:\", scores.mean())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0e1e8d0c",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bf3dc55d",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bbf8dbca",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "id": "ffb4bd27",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[132  14]\n",
      " [ 25  57]]\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.84      0.90      0.87       146\n",
      "           1       0.80      0.70      0.75        82\n",
      "\n",
      "    accuracy                           0.83       228\n",
      "   macro avg       0.82      0.80      0.81       228\n",
      "weighted avg       0.83      0.83      0.83       228\n",
      "\n"
     ]
    }
   ],
   "source": [
    "from sklearn.svm import SVC\n",
    "from imblearn.pipeline import Pipeline as ImbPipeline\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.metrics import classification_report, confusion_matrix\n",
    "from imblearn.over_sampling import SMOTE\n",
    "import pandas as pd\n",
    "\n",
    "\n",
    "X_train, X_test, y_train, y_test = train_test_split(list(df[\"text\"]), list(df[\"science_related\"]), test_size=0.2, random_state=42)\n",
    "\n",
    "# Pipeline avec SVM\n",
    "pipeline_svm = ImbPipeline([\n",
    "    ('vectorizer', TfidfVectorizer()),\n",
    "    ('smote', SMOTE(random_state=42)),\n",
    "    ('classifier', SVC(class_weight='balanced', kernel='linear', random_state=42))\n",
    "])\n",
    "\n",
    "# Entraînement\n",
    "pipeline_svm.fit(X_train, y_train)\n",
    "\n",
    "# Prédiction\n",
    "y_pred = pipeline_svm.predict(X_test)\n",
    "\n",
    "# Évaluation\n",
    "print(confusion_matrix(y_test, y_pred))\n",
    "print(classification_report(y_test, y_pred))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "id": "6e9f194b",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/cm/.local/lib/python3.8/site-packages/matplotlib/backends/backend_agg.py:238: RuntimeWarning: Glyph 1 missing from current font.\n",
      "  font.set_text(s, 0.0, flags=flags)\n",
      "/home/cm/.local/lib/python3.8/site-packages/matplotlib/backends/backend_agg.py:201: RuntimeWarning: Glyph 1 missing from current font.\n",
      "  font.set_text(s, 0, flags=flags)\n"
     ]
    }
   ],
   "source": [
    "# Réduire la dimension à 2D avec PCA\n",
    "pca = PCA(n_components=2)\n",
    "X_pca = pca.fit_transform(X_vec.toarray())  # Convertir la matrice creuse en tableau dense\n",
    "\n",
    "# Visualiser les données en 2D\n",
    "plt.figure(figsize=(10, 6))\n",
    "plt.scatter(X_pca[:, 0], X_pca[:, 1], c=y_binary, cmap='viridis', alpha=0.6)\n",
    "plt.colorbar(label='science_related')\n",
    "plt.title('Visualisation des tweets vectorisés (PCA 2D)')\n",
    "plt.xlabel('Composante principale 1')\n",
    "plt.ylabel('Composante principale 2')\n",
    "\n",
    "plt.axvline(x=-0.06, color='red', linestyle='--', linewidth=2)  \n",
    "plt.axvline(x=0, color='red', linestyle='--', linewidth=2)  \n",
    "\n",
    "plt.axhline(y=0.07, color='red', linestyle='--')\n",
    "plt.axhline(y=-0.05, color='red', linestyle='--')\n",
    "\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "982dd57a",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 156,
   "id": "04271f20",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "1140"
      ]
     },
     "execution_count": 156,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(refined_tweets)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c1018ded",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ee5748a2",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 157,
   "id": "7e3bc9e1",
   "metadata": {},
   "outputs": [],
   "source": [
    "pca_df = pd.DataFrame({\n",
    "    'pca1': X_pca[:, 0],\n",
    "    'pca2': X_pca[:, 1],\n",
    "    'label': y_binary,\n",
    "    'tweet': filtered_words  # doit être dans le même ordre que X_train\n",
    "})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 105,
   "id": "8689dfa5",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "-0.08623697698364022 0.2517037492972924\n",
      "-0.14729107119292217 0.7191086346164846\n"
     ]
    }
   ],
   "source": [
    "print(pca_df['pca1'].min(), pca_df['pca1'].max())\n",
    "print(pca_df['pca2'].min(), pca_df['pca2'].max())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 106,
   "id": "80aa7257",
   "metadata": {},
   "outputs": [],
   "source": [
    "zone = pca_df[\n",
    "    (pca_df['pca2'] > -0.05) & (pca_df['pca2'] < 0.07) &  # limites vertical\n",
    "    (pca_df['pca1'] > -0.06) & (pca_df['pca1'] < 0.0)    # limites horizontal\n",
    "]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 108,
   "id": "672352df",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "446\n",
      "303\n"
     ]
    }
   ],
   "source": [
    "print(len(zone[zone[\"label\"] == 0]))\n",
    "print(len(zone[zone[\"label\"] == 1]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 109,
   "id": "382a5588",
   "metadata": {},
   "outputs": [],
   "source": [
    "zone.to_csv(\"pca_zone.csv\", index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 110,
   "id": "9882c0ac",
   "metadata": {},
   "outputs": [],
   "source": [
    "df[df[\"science_related\"] == 1].to_csv(\"science_related.csv\", index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ba8aa3e4",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "dc2ba90e",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2ffaad1d",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 303,
   "id": "4d3a0154",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Réduire la dimension à 2D avec t-SNE\n",
    "tsne = TSNE(n_components=3, random_state=42, perplexity=15)\n",
    "X_train_tsne = tsne.fit_transform(X_train_vec.toarray())\n",
    "\n",
    "# Visualiser les données en 2D\n",
    "plt.figure(figsize=(10, 6))\n",
    "plt.scatter(X_train_tsne[:, 0], X_train_tsne[:, 1], c=y_train, cmap='viridis', alpha=0.6)\n",
    "plt.colorbar(label='science_related')\n",
    "plt.title('Visualisation des tweets vectorisés (t-SNE 2D)')\n",
    "plt.xlabel('Composante t-SNE 1')\n",
    "plt.ylabel('Composante t-SNE 2')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f5ce465c",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "158f911b",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "56bda7aa",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fc619bb3",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
