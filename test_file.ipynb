{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "656b0e68",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "from gensim.models import Word2Vec\n",
    "import nltk\n",
    "import re\n",
    "from nltk import word_tokenize, pos_tag\n",
    "from nltk.corpus import words\n",
    "from nltk.stem import WordNetLemmatizer\n",
    "from nltk.corpus import wordnet\n",
    "import enchant\n",
    "import numpy as np\n",
    "from collections import Counter\n",
    "import seaborn as sns\n",
    "\n",
    "\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "from sklearn.model_selection import train_test_split\n",
    "from imblearn.over_sampling import SMOTE\n",
    "import random\n",
    "from sklearn.model_selection import KFold\n",
    "\n",
    "\n",
    "\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from sklearn.svm import SVC\n",
    "from sklearn.metrics import classification_report, accuracy_score\n",
    "from sklearn.model_selection import cross_val_score\n",
    "\n",
    "from sklearn.decomposition import PCA\n",
    "import matplotlib.pyplot as plt\n",
    "from sklearn.manifold import TSNE\n",
    "\n",
    "import emoji\n",
    "from scipy.sparse import hstack\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from nltk.corpus import stopwords\n",
    "\n",
    "\n",
    "\n",
    "#nltk.download('punkt_tab')\n",
    "#nltk.download(\"words\")\n",
    "#nltk.download('wordnet')\n",
    "#nltk.download('averaged_perceptron_tagger_eng')\n",
    "#%matplotlib qt\n",
    "#nltk.download('stopwords')\n",
    "%matplotlib inline\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "d3dee810",
   "metadata": {},
   "outputs": [],
   "source": [
    "df = pd.read_csv(\"scitweets_export.tsv\", sep = \"\\t\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "fe10be0e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "accuracy: [0.71929825 0.73684211 0.83333333 0.79824561 0.76315789 0.75438596\n",
      " 0.83333333 0.81578947 0.78070175 0.8245614 ]\n",
      "Moyenne: 0.7859649122807018\n",
      "---------------------------------------------------------------------------------\n",
      "\n",
      "F1 scores: [0.30434783 0.34782609 0.6779661  0.61016949 0.52631579 0.51724138\n",
      " 0.74666667 0.69565217 0.62686567 0.6875    ]\n",
      "Moyenne: 0.5740551187269347\n"
     ]
    }
   ],
   "source": [
    "# ------------------------------------------------------------\n",
    "# SOLUTION FINALE POUR CLASSIFICATION MULTI-LABEL\n",
    "# Avec gestion du déséquilibre et validation croisée adaptée\n",
    "# ------------------------------------------------------------\n",
    "\"\"\"le Hamming loss est une métrique utilisée en classification multi-label (comme dans ton cas), \n",
    "et elle mesure la proportion de mauvaises prédictions de labels par rapport au nombre total de labels.\n",
    "\n",
    "\"\"\"\n",
    "\n",
    "import numpy as np\n",
    "from sklearn.model_selection import train_test_split, KFold\n",
    "from sklearn.metrics import classification_report, f1_score\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.svm import LinearSVC\n",
    "from sklearn.multioutput import MultiOutputClassifier\n",
    "from sklearn.model_selection import GridSearchCV\n",
    "from scipy.sparse import vstack\n",
    "from collections import Counter\n",
    "from sklearn.ensemble import GradientBoostingClassifier\n",
    "from sklearn.metrics import accuracy_score\n",
    "from sklearn.metrics import confusion_matrix, ConfusionMatrixDisplay\n",
    "\n",
    "\n",
    "# 1. Préparation des données\n",
    "\n",
    "stop_words = set(stopwords.words('english'))\n",
    "custom_stop_words = {\"rt\", \"co\", \"amp\", \"via\"}\n",
    "#negations = {\"not\", \"no\", \"nor\", \"neither\", \"never\", \"none\"}\n",
    "stop_words = stop_words #- negations\n",
    "stop_words.update(custom_stop_words)\n",
    "lemmatizer = WordNetLemmatizer()\n",
    "\n",
    "def preprocess_tweet(tweet):\n",
    "    #tweet = re.sub(r\"@\\w+|\\W\", \" \", tweet)\n",
    "    tweet = re.sub(r\"(https?://\\S+)\", \" URL \", tweet)\n",
    "    tweet = tweet.lower()\n",
    "    tokens = word_tokenize(tweet)\n",
    "    tokens = [lemmatizer.lemmatize(word) for word in tokens if word not in stop_words]\n",
    "    return \" \".join(tokens)\n",
    "\n",
    "# 2. Chargement et préparation\n",
    "sci_df = df[df['science_related'] == 1].copy()\n",
    "sci_df['cleaned_text'] = sci_df['text'].apply(preprocess_tweet)\n",
    "\n",
    "# 3. Création des cibles multi-labels\n",
    "y = sci_df[['scientific_claim', 'scientific_reference', 'scientific_context']].values\n",
    "X = text[\"df\"]\n",
    "\n",
    "\n",
    "\n",
    "# 4. Vectorisation\n",
    "vectorizer = TfidfVectorizer(\n",
    "    ngram_range=(1, 3),\n",
    "    max_features=8000,\n",
    "    min_df=3,\n",
    "    max_df=0.85\n",
    ")\n",
    "X_vec = vectorizer.fit_transform(X)\n",
    "\n",
    "feature_names = np.array(vectorizer.get_feature_names_out())\n",
    "\n",
    "# 5. Séparation train/test\n",
    "X_train, X_test, y_train, y_test = train_test_split(\n",
    "    X_vec, y, \n",
    "    test_size=0.2,\n",
    "    random_state=42\n",
    ")\n",
    "\n",
    "# 6. Rééchantillonnage manuel adapté au multi-label\n",
    "def multilabel_oversample(X, y, random_state=None):\n",
    "    np.random.seed(random_state)\n",
    "    \n",
    "    # Compter les occurrences de chaque combinaison de labels\n",
    "    unique_labels, counts = np.unique(y, axis=0, return_counts=True)\n",
    "    max_count = max(counts)\n",
    "    \n",
    "    resampled_X = []\n",
    "    resampled_y = []\n",
    "    \n",
    "    for label_combination, count in zip(unique_labels, counts):\n",
    "        indices = np.where((y == label_combination).all(axis=1))[0]\n",
    "        \n",
    "        # Suréchantillonnage seulement pour les classes minoritaires\n",
    "        if count < max_count:\n",
    "            n_to_add = max_count - count\n",
    "            selected = np.random.choice(indices, size=n_to_add, replace=True)\n",
    "            \n",
    "            resampled_X.append(X[selected])\n",
    "            resampled_y.append(y[selected])\n",
    "    \n",
    "    if resampled_X:\n",
    "        return vstack([X] + resampled_X), np.vstack([y] + resampled_y)\n",
    "    return X, y\n",
    "\n",
    "X_train_res, y_train_res = multilabel_oversample(X_train, y_train, random_state=42)\n",
    "\n",
    "# 7. Configuration des modèles\n",
    "models = {\n",
    "    \"LinearSVC\": {\n",
    "        \"model\": LinearSVC(dual=False, class_weight='balanced'),\n",
    "        \"params\": {\n",
    "            'estimator__C': [0.1, 1],\n",
    "            'estimator__penalty': ['l1', 'l2']\n",
    "        }\n",
    "    },\n",
    "    \"RandomForest\": {\n",
    "        \"model\": RandomForestClassifier(class_weight='balanced_subsample'),\n",
    "        \"params\": {\n",
    "            'estimator__n_estimators': [100],\n",
    "            'estimator__max_depth': [10, None],\n",
    "            'estimator__min_samples_split': [5, 10]\n",
    "        }\n",
    "    },\n",
    "    \"SVM\": {\n",
    "        \"model\": SVC(probability=True),\n",
    "        \"params\": {\n",
    "            'estimator__C': [0.1, 1, 10],\n",
    "            'estimator__kernel': ['linear', 'rbf']\n",
    "        }\n",
    "    },\n",
    "    \"Naive Bayes\": {\n",
    "        \"model\": MultinomialNB(),\n",
    "        \"params\": {\n",
    "            'estimator__alpha': [0.1, 0.5, 1.0],\n",
    "            'estimator__fit_prior': [True, False]\n",
    "        }\n",
    "    },\n",
    "    \"k-NN\": {\n",
    "        \"model\": KNeighborsClassifier(),\n",
    "        \"params\": {\n",
    "            'estimator__n_neighbors': [3, 5, 7],\n",
    "            'estimator__weights': ['uniform', 'distance'],\n",
    "            'estimator__metric': ['euclidean', 'cosine']\n",
    "        }\n",
    "    },\n",
    "    \n",
    "    \"Logistic Regression\": {\n",
    "        \"model\": LogisticRegression(max_iter=1000),\n",
    "        \"params\": {\n",
    "            'estimator__C': [0.1, 1, 10],\n",
    "            'estimator__penalty': ['l1', 'l2'],\n",
    "            'estimator__solver': ['liblinear']\n",
    "        }\n",
    "    },\n",
    "\n",
    "    \"Gradient Boosting\": {\n",
    "        \"model\": GradientBoostingClassifier(random_state=42),\n",
    "        \"params\": {\n",
    "            'estimator__n_estimators': [50, 100],\n",
    "            'estimator__learning_rate': [0.01, 0.1],\n",
    "            'estimator__max_depth': [3, 5]\n",
    "        }\n",
    "    }\n",
    "    \n",
    "}\n",
    "\n",
    "# 8. Évaluation avec KFold standard (adapté au multi-label)\n",
    "kf = KFold(n_splits=10, shuffle=True, random_state=42)\n",
    "\n",
    "for name, config in models.items():\n",
    "    print(f\"\\n{'='*60}\")\n",
    "    print(f\"Optimisation de {name}\")\n",
    "    print(f\"{'='*60}\")\n",
    "    \n",
    "    model = MultiOutputClassifier(config['model'])\n",
    "    \n",
    "    grid = GridSearchCV(\n",
    "        model,\n",
    "        param_grid=config['params'],\n",
    "        cv=kf,\n",
    "        scoring='f1_micro',\n",
    "        n_jobs=-1,\n",
    "        verbose=1\n",
    "    )\n",
    "    \n",
    "    # Entraînement sur données rééchantillonnées\n",
    "    grid.fit(X_train_res, y_train_res)\n",
    "    \n",
    "    # Évaluation sur test set original\n",
    "    y_pred = grid.predict(X_test)\n",
    "    \n",
    "    print(f\"\\nMeilleurs paramètres: {grid.best_params_}\")\n",
    "    print(\"\\nPerformance sur le TEST SET:\")\n",
    "    print(classification_report(\n",
    "        y_test, y_pred,\n",
    "        target_names=['claim', 'reference', 'context'],\n",
    "        zero_division=0\n",
    "    ))\n",
    "    \n",
    "    ###################################################################\n",
    "    \n",
    "    # Afficher une matrice de confusion\n",
    "    \n",
    "    # Noms des étiquettes pour chaque colonne\n",
    "    label_names = ['claim', 'reference', 'context']\n",
    "\n",
    "    # Boucle sur chaque label (chaque colonne)\n",
    "    for i, label in enumerate(label_names):\n",
    "        cm = confusion_matrix(y_test[:, i], y_pred[:, i])\n",
    "        disp = ConfusionMatrixDisplay(confusion_matrix=cm, display_labels=[0, 1])\n",
    "        disp.plot(cmap='Reds')\n",
    "        plt.title(f\"Matrice de confusion - {label} - Modèle: {name}\")\n",
    "        plt.show()\n",
    "    \n",
    "    \n",
    "    \n",
    "    ###################################################################\n",
    "    ###################################################################\n",
    "    \n",
    "    #Extraire les features les plus important\n",
    "    \n",
    "    nombre_de_features = 50\n",
    "    \n",
    "    print(f\"\\nTop\", nombre_de_features, \"features importantes pour le modèle: {name}\")\n",
    "\n",
    "    # Récupérer le meilleur modèle entraîné\n",
    "    best_model = grid.best_estimator_\n",
    "\n",
    "    # Pour chaque sortie (label multi-label), extraire les features importantes\n",
    "    for i, label in enumerate(['claim', 'reference', 'context']):\n",
    "        print(f\"\\n--- Top features pour la classe '{label}' ---\")\n",
    "\n",
    "        estimator = best_model.estimators_[i]  # modèle pour ce label\n",
    "\n",
    "        if hasattr(estimator, 'coef_'):\n",
    "            # Pour LinearSVC ou LogisticRegression\n",
    "            coefs = estimator.coef_.flatten()\n",
    "            top_indices = np.argsort(np.abs(coefs))[(-1*nombre_de_features):][::-1]\n",
    "            top_features = feature_names[top_indices]\n",
    "            #print(list(zip(top_features, coefs[top_indices])))\n",
    "            print(list(zip(top_features)))\n",
    "\n",
    "\n",
    "        elif hasattr(estimator, 'feature_importances_'):\n",
    "            # Pour RandomForest, GradientBoosting\n",
    "            importances = estimator.feature_importances_\n",
    "            top_indices = np.argsort(importances)[(-1*nombre_de_features):][::-1]\n",
    "            top_features = feature_names[top_indices]\n",
    "            #print(list(zip(top_features, importances[top_indices])))\n",
    "            print(list(zip(top_features)))\n",
    "\n",
    "        else:\n",
    "            print(f\"Pas d'attribut de feature importance pour {type(estimator).__name__}\")\n",
    "\n",
    "    \n",
    "    \n",
    "    \n",
    "    ####################################################################\n",
    "    \n",
    "    # Scores de validation croisée\n",
    "    cv_scores = []\n",
    "    cv_score = []\n",
    "    for train_idx, test_idx in kf.split(X_train_res):\n",
    "        X_train_fold, X_val_fold = X_train_res[train_idx], X_train_res[test_idx]\n",
    "        y_train_fold, y_val_fold = y_train_res[train_idx], y_train_res[test_idx]\n",
    "        \n",
    "        model.set_params(**grid.best_params_)\n",
    "        model.fit(X_train_fold, y_train_fold)\n",
    "        y_pred_fold = model.predict(X_val_fold)\n",
    "        \n",
    "        # Calcul de l'accuracy au lieu du F1-score\n",
    "        cv_scores.append(accuracy_score(y_val_fold, y_pred_fold))\n",
    "        cv_score.append(f1_score(y_val_fold, y_pred_fold, average='micro'))\n",
    "    \n",
    "    print(f\"\\nCV F1-Score (micro): {np.mean(cv_score):.4f} ± {np.std(cv_scores):.4f}\")\n",
    "    print(f\"\\n accuracy : {np.mean(cv_scores):.4f} ± {np.std(cv_scores):.4f}\")\n",
    "    from sklearn.metrics import hamming_loss\n",
    "    print(\"Hamming Loss:\", hamming_loss(y_test, y_pred))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "30852031",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
