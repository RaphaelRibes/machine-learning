{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "9fc29ebf-666d-4180-ab9a-2a56e26348dd",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package wordnet to /home/tiziri-\n",
      "[nltk_data]     tamani/nltk_data...\n",
      "[nltk_data]   Package wordnet is already up-to-date!\n",
      "[nltk_data] Downloading package punkt to /home/tiziri-\n",
      "[nltk_data]     tamani/nltk_data...\n",
      "[nltk_data]   Package punkt is already up-to-date!\n",
      "[nltk_data] Downloading package averaged_perceptron_tagger to\n",
      "[nltk_data]     /home/tiziri-tamani/nltk_data...\n",
      "[nltk_data]   Package averaged_perceptron_tagger is already up-to-\n",
      "[nltk_data]       date!\n",
      "[nltk_data] Downloading package stopwords to /home/tiziri-\n",
      "[nltk_data]     tamani/nltk_data...\n",
      "[nltk_data]   Package stopwords is already up-to-date!\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "import sklearn as sk\n",
    "import nltk\n",
    "\n",
    "nltk.download('wordnet')\n",
    "nltk.download('punkt')\n",
    "nltk.download('averaged_perceptron_tagger')\n",
    "nltk.download('stopwords')\n",
    "\n",
    "from nltk.corpus import stopwords\n",
    "import pandas as pd\n",
    "import re\n",
    "from nltk.corpus import stopwords\n",
    "from nltk.tokenize import word_tokenize\n",
    "from nltk.stem import WordNetLemmatizer\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "from sklearn.model_selection import train_test_split, GridSearchCV, KFold, cross_val_score\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.naive_bayes import MultinomialNB\n",
    "from sklearn.neighbors import KNeighborsClassifier\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from sklearn.svm import SVC\n",
    "from sklearn.metrics import classification_report\n",
    "from sklearn.decomposition import PCA\n",
    "import matplotlib.pyplot as plt\n",
    "from imblearn.over_sampling import SMOTE\n",
    "import nltk"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "f187fa1d-bf86-4231-89ed-db90fda55e13",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Unnamed: 0</th>\n",
       "      <th>tweet_id</th>\n",
       "      <th>text</th>\n",
       "      <th>science_related</th>\n",
       "      <th>scientific_claim</th>\n",
       "      <th>scientific_reference</th>\n",
       "      <th>scientific_context</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0</td>\n",
       "      <td>3,16669998137483E+017</td>\n",
       "      <td>Knees are a bit sore. i guess that's a sign th...</td>\n",
       "      <td>0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>1</td>\n",
       "      <td>3,19090866545386E+017</td>\n",
       "      <td>McDonald's breakfast stop then the gym üèÄüí™</td>\n",
       "      <td>0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>2</td>\n",
       "      <td>3,22030931022066E+017</td>\n",
       "      <td>Can any Gynecologist with Cancer Experience ex...</td>\n",
       "      <td>1</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>3</td>\n",
       "      <td>3,22694830620807E+017</td>\n",
       "      <td>Couch-lock highs lead to sleeping in the couch...</td>\n",
       "      <td>1</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>4</td>\n",
       "      <td>3,28524426658329E+017</td>\n",
       "      <td>Does daily routine help prevent problems with ...</td>\n",
       "      <td>1</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   Unnamed: 0               tweet_id  \\\n",
       "0           0  3,16669998137483E+017   \n",
       "1           1  3,19090866545386E+017   \n",
       "2           2  3,22030931022066E+017   \n",
       "3           3  3,22694830620807E+017   \n",
       "4           4  3,28524426658329E+017   \n",
       "\n",
       "                                                text  science_related  \\\n",
       "0  Knees are a bit sore. i guess that's a sign th...                0   \n",
       "1          McDonald's breakfast stop then the gym üèÄüí™                0   \n",
       "2  Can any Gynecologist with Cancer Experience ex...                1   \n",
       "3  Couch-lock highs lead to sleeping in the couch...                1   \n",
       "4  Does daily routine help prevent problems with ...                1   \n",
       "\n",
       "   scientific_claim  scientific_reference  scientific_context  \n",
       "0               0.0                   0.0                 0.0  \n",
       "1               0.0                   0.0                 0.0  \n",
       "2               1.0                   0.0                 0.0  \n",
       "3               1.0                   0.0                 0.0  \n",
       "4               1.0                   0.0                 0.0  "
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df = pd.read_csv('scitweets_export.tsv', sep='\\t')\n",
    "df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "74afca4a-822f-4c88-a8ee-2fcecdaa0dda",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "============================================================\n",
      "Optimisation de LinearSVC\n",
      "============================================================\n",
      "Fitting 10 folds for each of 4 candidates, totalling 40 fits\n",
      "\n",
      "Meilleurs param√®tres: {'estimator__C': 1, 'estimator__penalty': 'l1'}\n",
      "\n",
      "Performance sur le TEST SET:\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "       claim       0.83      0.80      0.81        54\n",
      "   reference       0.67      0.74      0.71        39\n",
      "     context       0.87      0.71      0.78        48\n",
      "\n",
      "   micro avg       0.79      0.75      0.77       141\n",
      "   macro avg       0.79      0.75      0.77       141\n",
      "weighted avg       0.80      0.75      0.77       141\n",
      " samples avg       0.84      0.82      0.78       141\n",
      "\n",
      "\n",
      "CV F1-Score (micro): 0.8838 ¬± 0.0318\n",
      "\n",
      "============================================================\n",
      "Optimisation de RandomForest\n",
      "============================================================\n",
      "Fitting 10 folds for each of 4 candidates, totalling 40 fits\n",
      "\n",
      "Meilleurs param√®tres: {'estimator__max_depth': None, 'estimator__min_samples_split': 5, 'estimator__n_estimators': 100}\n",
      "\n",
      "Performance sur le TEST SET:\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "       claim       0.84      0.80      0.82        54\n",
      "   reference       0.68      0.77      0.72        39\n",
      "     context       0.83      0.92      0.87        48\n",
      "\n",
      "   micro avg       0.79      0.83      0.81       141\n",
      "   macro avg       0.79      0.83      0.80       141\n",
      "weighted avg       0.79      0.83      0.81       141\n",
      " samples avg       0.84      0.87      0.81       141\n",
      "\n",
      "\n",
      "CV F1-Score (micro): 0.9148 ¬± 0.0224\n",
      "\n",
      "============================================================\n",
      "Optimisation de SVM\n",
      "============================================================\n",
      "Fitting 10 folds for each of 6 candidates, totalling 60 fits\n",
      "\n",
      "Meilleurs param√®tres: {'estimator__C': 10, 'estimator__kernel': 'rbf'}\n",
      "\n",
      "Performance sur le TEST SET:\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "       claim       0.81      0.89      0.85        54\n",
      "   reference       0.64      0.82      0.72        39\n",
      "     context       0.74      0.96      0.84        48\n",
      "\n",
      "   micro avg       0.74      0.89      0.81       141\n",
      "   macro avg       0.73      0.89      0.80       141\n",
      "weighted avg       0.74      0.89      0.81       141\n",
      " samples avg       0.77      0.92      0.80       141\n",
      "\n",
      "\n",
      "CV F1-Score (micro): 0.8985 ¬± 0.0263\n",
      "\n",
      "============================================================\n",
      "Optimisation de Naive Bayes\n",
      "============================================================\n",
      "Fitting 10 folds for each of 6 candidates, totalling 60 fits\n",
      "\n",
      "Meilleurs param√®tres: {'estimator__alpha': 0.5, 'estimator__fit_prior': True}\n",
      "\n",
      "Performance sur le TEST SET:\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "       claim       0.85      0.85      0.85        54\n",
      "   reference       0.58      0.64      0.61        39\n",
      "     context       0.71      0.96      0.81        48\n",
      "\n",
      "   micro avg       0.72      0.83      0.77       141\n",
      "   macro avg       0.71      0.82      0.76       141\n",
      "weighted avg       0.73      0.83      0.77       141\n",
      " samples avg       0.74      0.86      0.75       141\n",
      "\n",
      "\n",
      "CV F1-Score (micro): 0.8609 ¬± 0.0296\n",
      "\n",
      "============================================================\n",
      "Optimisation de k-NN\n",
      "============================================================\n",
      "Fitting 10 folds for each of 12 candidates, totalling 120 fits\n",
      "\n",
      "Meilleurs param√®tres: {'estimator__metric': 'cosine', 'estimator__n_neighbors': 7, 'estimator__weights': 'distance'}\n",
      "\n",
      "Performance sur le TEST SET:\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "       claim       0.86      0.69      0.76        54\n",
      "   reference       0.56      0.62      0.59        39\n",
      "     context       0.66      0.94      0.78        48\n",
      "\n",
      "   micro avg       0.69      0.75      0.72       141\n",
      "   macro avg       0.69      0.75      0.71       141\n",
      "weighted avg       0.71      0.75      0.72       141\n",
      " samples avg       0.71      0.78      0.68       141\n",
      "\n",
      "\n",
      "CV F1-Score (micro): 0.8601 ¬± 0.0353\n",
      "\n",
      "============================================================\n",
      "Optimisation de Logistic Regression\n",
      "============================================================\n",
      "Fitting 10 folds for each of 6 candidates, totalling 60 fits\n",
      "\n",
      "Meilleurs param√®tres: {'estimator__C': 10, 'estimator__penalty': 'l1', 'estimator__solver': 'liblinear'}\n",
      "\n",
      "Performance sur le TEST SET:\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "       claim       0.82      0.83      0.83        54\n",
      "   reference       0.70      0.72      0.71        39\n",
      "     context       0.87      0.81      0.84        48\n",
      "\n",
      "   micro avg       0.80      0.79      0.80       141\n",
      "   macro avg       0.79      0.79      0.79       141\n",
      "weighted avg       0.80      0.79      0.80       141\n",
      " samples avg       0.85      0.86      0.81       141\n",
      "\n",
      "\n",
      "CV F1-Score (micro): 0.8904 ¬± 0.0275\n",
      "\n",
      "============================================================\n",
      "Optimisation de Gradient Boosting\n",
      "============================================================\n",
      "Fitting 10 folds for each of 8 candidates, totalling 80 fits\n",
      "\n",
      "Meilleurs param√®tres: {'estimator__learning_rate': 0.1, 'estimator__max_depth': 5, 'estimator__n_estimators': 100}\n",
      "\n",
      "Performance sur le TEST SET:\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "       claim       0.82      0.87      0.85        54\n",
      "   reference       0.69      0.74      0.72        39\n",
      "     context       0.82      0.83      0.82        48\n",
      "\n",
      "   micro avg       0.78      0.82      0.80       141\n",
      "   macro avg       0.78      0.82      0.80       141\n",
      "weighted avg       0.78      0.82      0.80       141\n",
      " samples avg       0.82      0.88      0.80       141\n",
      "\n",
      "\n",
      "CV F1-Score (micro): 0.9049 ¬± 0.0280\n"
     ]
    }
   ],
   "source": [
    "# ------------------------------------------------------------\n",
    "# SOLUTION FINALE POUR CLASSIFICATION MULTI-LABEL\n",
    "# Avec gestion du d√©s√©quilibre et validation crois√©e adapt√©e\n",
    "# ------------------------------------------------------------\n",
    "\n",
    "import numpy as np\n",
    "from sklearn.model_selection import train_test_split, KFold\n",
    "from sklearn.metrics import classification_report, f1_score\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.svm import LinearSVC\n",
    "from sklearn.multioutput import MultiOutputClassifier\n",
    "from sklearn.model_selection import GridSearchCV\n",
    "from scipy.sparse import vstack\n",
    "from collections import Counter\n",
    "from sklearn.ensemble import GradientBoostingClassifier\n",
    "\n",
    "\n",
    "# 1. Pr√©paration des donn√©es\n",
    "stop_words = set(stopwords.words('english'))\n",
    "custom_stop_words = {\"http\", \"https\", \"rt\", \"co\", \"amp\", \"via\"}\n",
    "stop_words.update(custom_stop_words)\n",
    "lemmatizer = WordNetLemmatizer()\n",
    "\n",
    "def preprocess_tweet(tweet):\n",
    "    tweet = tweet.lower()\n",
    "    tokens = word_tokenize(tweet)\n",
    "    tokens = [lemmatizer.lemmatize(word) for word in tokens if word not in stop_words]\n",
    "    return \" \".join(tokens)\n",
    "\n",
    "# 2. Chargement et pr√©paration\n",
    "sci_df = df[df['science_related'] == 1].copy()\n",
    "sci_df['cleaned_text'] = sci_df['text'].apply(preprocess_tweet)\n",
    "\n",
    "# 3. Cr√©ation des cibles multi-labels\n",
    "y = sci_df[['scientific_claim', 'scientific_reference', 'scientific_context']].values\n",
    "X = sci_df['cleaned_text']\n",
    "\n",
    "# 4. Vectorisation\n",
    "vectorizer = TfidfVectorizer(\n",
    "    ngram_range=(1, 2),\n",
    "    max_features=8000,\n",
    "    min_df=3,\n",
    "    max_df=0.85\n",
    ")\n",
    "X_vec = vectorizer.fit_transform(X)\n",
    "\n",
    "# 5. S√©paration train/test\n",
    "X_train, X_test, y_train, y_test = train_test_split(\n",
    "    X_vec, y, \n",
    "    test_size=0.2,\n",
    "    random_state=42\n",
    ")\n",
    "\n",
    "# 6. R√©√©chantillonnage manuel adapt√© au multi-label\n",
    "def multilabel_oversample(X, y, random_state=None):\n",
    "    np.random.seed(random_state)\n",
    "    \n",
    "    # Compter les occurrences de chaque combinaison de labels\n",
    "    unique_labels, counts = np.unique(y, axis=0, return_counts=True)\n",
    "    max_count = max(counts)\n",
    "    \n",
    "    resampled_X = []\n",
    "    resampled_y = []\n",
    "    \n",
    "    for label_combination, count in zip(unique_labels, counts):\n",
    "        indices = np.where((y == label_combination).all(axis=1))[0]\n",
    "        \n",
    "        # Sur√©chantillonnage seulement pour les classes minoritaires\n",
    "        if count < max_count:\n",
    "            n_to_add = max_count - count\n",
    "            selected = np.random.choice(indices, size=n_to_add, replace=True)\n",
    "            \n",
    "            resampled_X.append(X[selected])\n",
    "            resampled_y.append(y[selected])\n",
    "    \n",
    "    if resampled_X:\n",
    "        return vstack([X] + resampled_X), np.vstack([y] + resampled_y)\n",
    "    return X, y\n",
    "\n",
    "X_train_res, y_train_res = multilabel_oversample(X_train, y_train, random_state=42)\n",
    "\n",
    "# 7. Configuration des mod√®les\n",
    "models = {\n",
    "    \"LinearSVC\": {\n",
    "        \"model\": LinearSVC(dual=False, class_weight='balanced'),\n",
    "        \"params\": {\n",
    "            'estimator__C': [0.1, 1],\n",
    "            'estimator__penalty': ['l1', 'l2']\n",
    "        }\n",
    "    },\n",
    "    \"RandomForest\": {\n",
    "        \"model\": RandomForestClassifier(class_weight='balanced_subsample'),\n",
    "        \"params\": {\n",
    "            'estimator__n_estimators': [100],\n",
    "            'estimator__max_depth': [10, None],\n",
    "            'estimator__min_samples_split': [5, 10]\n",
    "        }\n",
    "    },\n",
    "    \"SVM\": {\n",
    "        \"model\": SVC(probability=True),\n",
    "        \"params\": {\n",
    "            'estimator__C': [0.1, 1, 10],\n",
    "            'estimator__kernel': ['linear', 'rbf']\n",
    "        }\n",
    "    },\n",
    "    \"Naive Bayes\": {\n",
    "        \"model\": MultinomialNB(),\n",
    "        \"params\": {\n",
    "            'estimator__alpha': [0.1, 0.5, 1.0],\n",
    "            'estimator__fit_prior': [True, False]\n",
    "        }\n",
    "    },\n",
    "    \"k-NN\": {\n",
    "        \"model\": KNeighborsClassifier(),\n",
    "        \"params\": {\n",
    "            'estimator__n_neighbors': [3, 5, 7],\n",
    "            'estimator__weights': ['uniform', 'distance'],\n",
    "            'estimator__metric': ['euclidean', 'cosine']\n",
    "        }\n",
    "    },\n",
    "    \n",
    "    \"Logistic Regression\": {\n",
    "        \"model\": LogisticRegression(max_iter=1000),\n",
    "        \"params\": {\n",
    "            'estimator__C': [0.1, 1, 10],\n",
    "            'estimator__penalty': ['l1', 'l2'],\n",
    "            'estimator__solver': ['liblinear']\n",
    "        }\n",
    "    },\n",
    "\n",
    "    \"Gradient Boosting\": {\n",
    "        \"model\": GradientBoostingClassifier(random_state=42),\n",
    "        \"params\": {\n",
    "            'estimator__n_estimators': [50, 100],\n",
    "            'estimator__learning_rate': [0.01, 0.1],\n",
    "            'estimator__max_depth': [3, 5]\n",
    "        }\n",
    "    }\n",
    "    \n",
    "}\n",
    "\n",
    "# 8. √âvaluation avec KFold standard (adapt√© au multi-label)\n",
    "kf = KFold(n_splits=10, shuffle=True, random_state=42)\n",
    "\n",
    "for name, config in models.items():\n",
    "    print(f\"\\n{'='*60}\")\n",
    "    print(f\"Optimisation de {name}\")\n",
    "    print(f\"{'='*60}\")\n",
    "    \n",
    "    model = MultiOutputClassifier(config['model'])\n",
    "    \n",
    "    grid = GridSearchCV(\n",
    "        model,\n",
    "        param_grid=config['params'],\n",
    "        cv=kf,\n",
    "        scoring='f1_micro',\n",
    "        n_jobs=-1,\n",
    "        verbose=1\n",
    "    )\n",
    "    \n",
    "    # Entra√Ænement sur donn√©es r√©√©chantillonn√©es\n",
    "    grid.fit(X_train_res, y_train_res)\n",
    "    \n",
    "    # √âvaluation sur test set original\n",
    "    y_pred = grid.predict(X_test)\n",
    "    \n",
    "    print(f\"\\nMeilleurs param√®tres: {grid.best_params_}\")\n",
    "    print(\"\\nPerformance sur le TEST SET:\")\n",
    "    print(classification_report(\n",
    "        y_test, y_pred,\n",
    "        target_names=['claim', 'reference', 'context'],\n",
    "        zero_division=0\n",
    "    ))\n",
    "    \n",
    "    # Scores de validation crois√©e\n",
    "    cv_scores = []\n",
    "    for train_idx, test_idx in kf.split(X_train_res):\n",
    "        X_train_fold, X_val_fold = X_train_res[train_idx], X_train_res[test_idx]\n",
    "        y_train_fold, y_val_fold = y_train_res[train_idx], y_train_res[test_idx]\n",
    "        \n",
    "        model.set_params(**grid.best_params_)\n",
    "        model.fit(X_train_fold, y_train_fold)\n",
    "        y_pred_fold = model.predict(X_val_fold)\n",
    "        \n",
    "        cv_scores.append(f1_score(y_val_fold, y_pred_fold, average='micro'))\n",
    "    \n",
    "    print(f\"\\nCV F1-Score (micro): {np.mean(cv_scores):.4f} ¬± {np.std(cv_scores):.4f}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "f0a9b8de-6027-4413-af8d-97bb66906351",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "============================================================\n",
      "Optimisation de LinearSVC\n",
      "============================================================\n",
      "Fitting 5 folds for each of 4 candidates, totalling 20 fits\n",
      "\n",
      "Meilleurs param√®tres: {'estimator__C': 0.1, 'estimator__penalty': 'l1'}\n",
      "\n",
      "Performance sur le TEST SET:\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "       claim       0.75      0.98      0.85        54\n",
      "   reference       0.67      0.92      0.77        39\n",
      "     context       0.64      1.00      0.78        48\n",
      "\n",
      "   micro avg       0.69      0.97      0.80       141\n",
      "   macro avg       0.68      0.97      0.80       141\n",
      "weighted avg       0.69      0.97      0.80       141\n",
      " samples avg       0.68      0.98      0.77       141\n",
      "\n",
      "\n",
      "CV F1-Score (micro): 0.8052 ¬± 0.0183\n",
      "\n",
      "============================================================\n",
      "Optimisation de RandomForest\n",
      "============================================================\n",
      "Fitting 5 folds for each of 4 candidates, totalling 20 fits\n",
      "\n",
      "Meilleurs param√®tres: {'estimator__max_depth': None, 'estimator__min_samples_split': 5, 'estimator__n_estimators': 100}\n",
      "\n",
      "Performance sur le TEST SET:\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "       claim       0.78      0.87      0.82        54\n",
      "   reference       0.68      0.77      0.72        39\n",
      "     context       0.80      0.85      0.83        48\n",
      "\n",
      "   micro avg       0.76      0.84      0.80       141\n",
      "   macro avg       0.76      0.83      0.79       141\n",
      "weighted avg       0.76      0.84      0.80       141\n",
      " samples avg       0.81      0.87      0.79       141\n",
      "\n",
      "\n",
      "CV F1-Score (micro): 0.8358 ¬± 0.0159\n",
      "\n",
      "============================================================\n",
      "Optimisation de SVM\n",
      "============================================================\n",
      "Fitting 5 folds for each of 6 candidates, totalling 30 fits\n",
      "\n",
      "Meilleurs param√®tres: {'estimator__C': 1, 'estimator__kernel': 'rbf'}\n",
      "\n",
      "Performance sur le TEST SET:\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "       claim       0.78      0.98      0.87        54\n",
      "   reference       0.63      0.85      0.73        39\n",
      "     context       0.68      1.00      0.81        48\n",
      "\n",
      "   micro avg       0.70      0.95      0.81       141\n",
      "   macro avg       0.70      0.94      0.80       141\n",
      "weighted avg       0.70      0.95      0.81       141\n",
      " samples avg       0.72      0.97      0.78       141\n",
      "\n",
      "\n",
      "CV F1-Score (micro): 0.8037 ¬± 0.0164\n",
      "\n",
      "============================================================\n",
      "Optimisation de Naive Bayes\n",
      "============================================================\n",
      "Fitting 5 folds for each of 6 candidates, totalling 30 fits\n",
      "\n",
      "Meilleurs param√®tres: {'estimator__alpha': 1.0, 'estimator__fit_prior': True}\n",
      "\n",
      "Performance sur le TEST SET:\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "       claim       0.78      0.96      0.86        54\n",
      "   reference       0.58      0.79      0.67        39\n",
      "     context       0.71      0.98      0.82        48\n",
      "\n",
      "   micro avg       0.70      0.92      0.80       141\n",
      "   macro avg       0.69      0.91      0.79       141\n",
      "weighted avg       0.70      0.92      0.80       141\n",
      " samples avg       0.72      0.94      0.78       141\n",
      "\n",
      "\n",
      "CV F1-Score (micro): 0.7960 ¬± 0.0225\n",
      "\n",
      "============================================================\n",
      "Optimisation de k-NN\n",
      "============================================================\n",
      "Fitting 5 folds for each of 12 candidates, totalling 60 fits\n",
      "\n",
      "Meilleurs param√®tres: {'estimator__metric': 'cosine', 'estimator__n_neighbors': 7, 'estimator__weights': 'uniform'}\n",
      "\n",
      "Performance sur le TEST SET:\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "       claim       0.86      0.89      0.87        54\n",
      "   reference       0.62      0.82      0.70        39\n",
      "     context       0.69      0.90      0.78        48\n",
      "\n",
      "   micro avg       0.72      0.87      0.79       141\n",
      "   macro avg       0.72      0.87      0.79       141\n",
      "weighted avg       0.73      0.87      0.79       141\n",
      " samples avg       0.76      0.90      0.77       141\n",
      "\n",
      "\n",
      "CV F1-Score (micro): 0.7837 ¬± 0.0199\n",
      "\n",
      "============================================================\n",
      "Optimisation de Logistic Regression\n",
      "============================================================\n",
      "Fitting 5 folds for each of 6 candidates, totalling 30 fits\n",
      "\n",
      "Meilleurs param√®tres: {'estimator__C': 1, 'estimator__penalty': 'l1', 'estimator__solver': 'liblinear'}\n",
      "\n",
      "Performance sur le TEST SET:\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "       claim       0.78      0.94      0.86        54\n",
      "   reference       0.68      0.72      0.70        39\n",
      "     context       0.81      0.92      0.86        48\n",
      "\n",
      "   micro avg       0.77      0.87      0.82       141\n",
      "   macro avg       0.76      0.86      0.81       141\n",
      "weighted avg       0.77      0.87      0.82       141\n",
      " samples avg       0.82      0.92      0.82       141\n",
      "\n",
      "\n",
      "CV F1-Score (micro): 0.8307 ¬± 0.0080\n",
      "\n",
      "============================================================\n",
      "Optimisation de Gradient Boosting\n",
      "============================================================\n",
      "Fitting 5 folds for each of 8 candidates, totalling 40 fits\n",
      "\n",
      "Meilleurs param√®tres: {'estimator__learning_rate': 0.01, 'estimator__max_depth': 3, 'estimator__n_estimators': 100}\n",
      "\n",
      "Performance sur le TEST SET:\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "       claim       0.74      0.98      0.84        54\n",
      "   reference       0.69      0.87      0.77        39\n",
      "     context       0.81      0.88      0.84        48\n",
      "\n",
      "   micro avg       0.75      0.91      0.82       141\n",
      "   macro avg       0.75      0.91      0.82       141\n",
      "weighted avg       0.75      0.91      0.82       141\n",
      " samples avg       0.80      0.94      0.82       141\n",
      "\n",
      "\n",
      "CV F1-Score (micro): 0.8410 ¬± 0.0137\n"
     ]
    }
   ],
   "source": [
    "# ------------------------------------------------------------\n",
    "# SOLUTION POUR CLASSIFICATION MULTI-LABEL SANS R√â√âQUILIBRAGE\n",
    "# ------------------------------------------------------------\n",
    "\n",
    "import numpy as np\n",
    "from sklearn.model_selection import train_test_split, KFold\n",
    "from sklearn.metrics import classification_report, f1_score\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.svm import LinearSVC, SVC\n",
    "from sklearn.naive_bayes import MultinomialNB\n",
    "from sklearn.neighbors import KNeighborsClassifier\n",
    "from sklearn.multioutput import MultiOutputClassifier\n",
    "from sklearn.model_selection import GridSearchCV\n",
    "from sklearn.ensemble import GradientBoostingClassifier\n",
    "\n",
    "# 1. Pr√©paration des donn√©es (identique)\n",
    "stop_words = set(stopwords.words('english'))\n",
    "custom_stop_words = {\"http\", \"https\", \"rt\", \"co\", \"amp\", \"via\"}\n",
    "stop_words.update(custom_stop_words)\n",
    "lemmatizer = WordNetLemmatizer()\n",
    "\n",
    "def preprocess_tweet(tweet):\n",
    "    tweet = tweet.lower()\n",
    "    tokens = word_tokenize(tweet)\n",
    "    tokens = [lemmatizer.lemmatize(word) for word in tokens if word not in stop_words]\n",
    "    return \" \".join(tokens)\n",
    "\n",
    "# 2. Chargement et pr√©paration\n",
    "sci_df = df[df['science_related'] == 1].copy()\n",
    "sci_df['cleaned_text'] = sci_df['text'].apply(preprocess_tweet)\n",
    "\n",
    "# 3. Cr√©ation des cibles multi-labels\n",
    "y = sci_df[['scientific_claim', 'scientific_reference', 'scientific_context']].values\n",
    "X = sci_df['cleaned_text']\n",
    "\n",
    "# 4. Vectorisation (identique)\n",
    "vectorizer = TfidfVectorizer(\n",
    "    ngram_range=(1, 2),\n",
    "    max_features=8000,\n",
    "    min_df=3,\n",
    "    max_df=0.85\n",
    ")\n",
    "X_vec = vectorizer.fit_transform(X)\n",
    "\n",
    "# 5. S√©paration train/test (identique)\n",
    "X_train, X_test, y_train, y_test = train_test_split(\n",
    "    X_vec, y, \n",
    "    test_size=0.2,\n",
    "    random_state=42\n",
    ")\n",
    "\n",
    "# 6. Configuration des mod√®les (simplifi√©e sans param√®tres de r√©√©quilibrage)\n",
    "models = {\n",
    "    \"LinearSVC\": {\n",
    "        \"model\": LinearSVC(dual=False),\n",
    "        \"params\": {\n",
    "            'estimator__C': [0.1, 1],\n",
    "            'estimator__penalty': ['l1', 'l2']\n",
    "        }\n",
    "    },\n",
    "    \"RandomForest\": {\n",
    "        \"model\": RandomForestClassifier(),\n",
    "        \"params\": {\n",
    "            'estimator__n_estimators': [100],\n",
    "            'estimator__max_depth': [10, None],\n",
    "            'estimator__min_samples_split': [5, 10]\n",
    "        }\n",
    "    },\n",
    "    \"SVM\": {\n",
    "        \"model\": SVC(probability=True),\n",
    "        \"params\": {\n",
    "            'estimator__C': [0.1, 1, 10],\n",
    "            'estimator__kernel': ['linear', 'rbf']\n",
    "        }\n",
    "    },\n",
    "    \"Naive Bayes\": {\n",
    "        \"model\": MultinomialNB(),\n",
    "        \"params\": {\n",
    "            'estimator__alpha': [0.1, 0.5, 1.0],\n",
    "            'estimator__fit_prior': [True, False]\n",
    "        }\n",
    "    },\n",
    "    \"k-NN\": {\n",
    "        \"model\": KNeighborsClassifier(),\n",
    "        \"params\": {\n",
    "            'estimator__n_neighbors': [3, 5, 7],\n",
    "            'estimator__weights': ['uniform', 'distance'],\n",
    "            'estimator__metric': ['euclidean', 'cosine']\n",
    "        }\n",
    "    },\n",
    "    \"Logistic Regression\": {\n",
    "        \"model\": LogisticRegression(max_iter=1000),\n",
    "        \"params\": {\n",
    "            'estimator__C': [0.1, 1, 10],\n",
    "            'estimator__penalty': ['l1', 'l2'],\n",
    "            'estimator__solver': ['liblinear']\n",
    "        }\n",
    "    },\n",
    "    \"Gradient Boosting\": {\n",
    "        \"model\": GradientBoostingClassifier(random_state=42),\n",
    "        \"params\": {\n",
    "            'estimator__n_estimators': [50, 100],\n",
    "            'estimator__learning_rate': [0.01, 0.1],\n",
    "            'estimator__max_depth': [3, 5]\n",
    "        }\n",
    "    }\n",
    "}\n",
    "\n",
    "# 7. √âvaluation avec KFold standard\n",
    "kf = KFold(n_splits=5, shuffle=True, random_state=42)\n",
    "\n",
    "for name, config in models.items():\n",
    "    print(f\"\\n{'='*60}\")\n",
    "    print(f\"Optimisation de {name}\")\n",
    "    print(f\"{'='*60}\")\n",
    "    \n",
    "    model = MultiOutputClassifier(config['model'])\n",
    "    \n",
    "    grid = GridSearchCV(\n",
    "        model,\n",
    "        param_grid=config['params'],\n",
    "        cv=kf,\n",
    "        scoring='f1_micro',\n",
    "        n_jobs=-1,\n",
    "        verbose=1\n",
    "    )\n",
    "    \n",
    "    # Entra√Ænement sur donn√©es originales (sans r√©√©chantillonnage)\n",
    "    grid.fit(X_train, y_train)\n",
    "    \n",
    "    # √âvaluation sur test set\n",
    "    y_pred = grid.predict(X_test)\n",
    "    \n",
    "    print(f\"\\nMeilleurs param√®tres: {grid.best_params_}\")\n",
    "    print(\"\\nPerformance sur le TEST SET:\")\n",
    "    print(classification_report(\n",
    "        y_test, y_pred,\n",
    "        target_names=['claim', 'reference', 'context'],\n",
    "        zero_division=0\n",
    "    ))\n",
    "    \n",
    "    # Scores de validation crois√©e\n",
    "    cv_scores = []\n",
    "    for train_idx, test_idx in kf.split(X_train):\n",
    "        X_train_fold, X_val_fold = X_train[train_idx], X_train[test_idx]\n",
    "        y_train_fold, y_val_fold = y_train[train_idx], y_train[test_idx]\n",
    "        \n",
    "        model.set_params(**grid.best_params_)\n",
    "        model.fit(X_train_fold, y_train_fold)\n",
    "        y_pred_fold = model.predict(X_val_fold)\n",
    "        \n",
    "        cv_scores.append(f1_score(y_val_fold, y_pred_fold, average='micro'))\n",
    "    \n",
    "    print(f\"\\nCV F1-Score (micro): {np.mean(cv_scores):.4f} ¬± {np.std(cv_scores):.4f}\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
