{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "9fc29ebf-666d-4180-ab9a-2a56e26348dd",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package wordnet to /home/tiziri-\n",
      "[nltk_data]     tamani/nltk_data...\n",
      "[nltk_data]   Package wordnet is already up-to-date!\n",
      "[nltk_data] Downloading package punkt to /home/tiziri-\n",
      "[nltk_data]     tamani/nltk_data...\n",
      "[nltk_data]   Package punkt is already up-to-date!\n",
      "[nltk_data] Downloading package averaged_perceptron_tagger to\n",
      "[nltk_data]     /home/tiziri-tamani/nltk_data...\n",
      "[nltk_data]   Package averaged_perceptron_tagger is already up-to-\n",
      "[nltk_data]       date!\n",
      "[nltk_data] Downloading package stopwords to /home/tiziri-\n",
      "[nltk_data]     tamani/nltk_data...\n",
      "[nltk_data]   Package stopwords is already up-to-date!\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "import sklearn as sk\n",
    "import nltk\n",
    "\n",
    "nltk.download('wordnet')\n",
    "nltk.download('punkt')\n",
    "nltk.download('averaged_perceptron_tagger')\n",
    "nltk.download('stopwords')\n",
    "\n",
    "from nltk.corpus import stopwords\n",
    "import pandas as pd\n",
    "import re\n",
    "from nltk.corpus import stopwords\n",
    "from nltk.tokenize import word_tokenize\n",
    "from nltk.stem import WordNetLemmatizer\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "from sklearn.model_selection import train_test_split, GridSearchCV, KFold, cross_val_score\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.naive_bayes import MultinomialNB\n",
    "from sklearn.neighbors import KNeighborsClassifier\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from sklearn.svm import SVC\n",
    "from sklearn.metrics import classification_report\n",
    "from sklearn.decomposition import PCA\n",
    "import matplotlib.pyplot as plt\n",
    "from imblearn.over_sampling import SMOTE\n",
    "import nltk"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "f187fa1d-bf86-4231-89ed-db90fda55e13",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Unnamed: 0</th>\n",
       "      <th>tweet_id</th>\n",
       "      <th>text</th>\n",
       "      <th>science_related</th>\n",
       "      <th>scientific_claim</th>\n",
       "      <th>scientific_reference</th>\n",
       "      <th>scientific_context</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0</td>\n",
       "      <td>3,16669998137483E+017</td>\n",
       "      <td>Knees are a bit sore. i guess that's a sign th...</td>\n",
       "      <td>0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>1</td>\n",
       "      <td>3,19090866545386E+017</td>\n",
       "      <td>McDonald's breakfast stop then the gym üèÄüí™</td>\n",
       "      <td>0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>2</td>\n",
       "      <td>3,22030931022066E+017</td>\n",
       "      <td>Can any Gynecologist with Cancer Experience ex...</td>\n",
       "      <td>1</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>3</td>\n",
       "      <td>3,22694830620807E+017</td>\n",
       "      <td>Couch-lock highs lead to sleeping in the couch...</td>\n",
       "      <td>1</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>4</td>\n",
       "      <td>3,28524426658329E+017</td>\n",
       "      <td>Does daily routine help prevent problems with ...</td>\n",
       "      <td>1</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   Unnamed: 0               tweet_id  \\\n",
       "0           0  3,16669998137483E+017   \n",
       "1           1  3,19090866545386E+017   \n",
       "2           2  3,22030931022066E+017   \n",
       "3           3  3,22694830620807E+017   \n",
       "4           4  3,28524426658329E+017   \n",
       "\n",
       "                                                text  science_related  \\\n",
       "0  Knees are a bit sore. i guess that's a sign th...                0   \n",
       "1          McDonald's breakfast stop then the gym üèÄüí™                0   \n",
       "2  Can any Gynecologist with Cancer Experience ex...                1   \n",
       "3  Couch-lock highs lead to sleeping in the couch...                1   \n",
       "4  Does daily routine help prevent problems with ...                1   \n",
       "\n",
       "   scientific_claim  scientific_reference  scientific_context  \n",
       "0               0.0                   0.0                 0.0  \n",
       "1               0.0                   0.0                 0.0  \n",
       "2               1.0                   0.0                 0.0  \n",
       "3               1.0                   0.0                 0.0  \n",
       "4               1.0                   0.0                 0.0  "
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df = pd.read_csv('scitweets_export.tsv', sep='\\t')\n",
    "df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 80,
   "id": "74afca4a-822f-4c88-a8ee-2fcecdaa0dda",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "============================================================\n",
      "Optimisation de LinearSVC\n",
      "============================================================\n",
      "Fitting 10 folds for each of 4 candidates, totalling 40 fits\n",
      "\n",
      "Meilleurs param√®tres: {'estimator__C': 1, 'estimator__penalty': 'l1'}\n",
      "\n",
      "Performance sur le TEST SET:\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "       claim       0.83      0.80      0.81        54\n",
      "   reference       0.67      0.72      0.69        39\n",
      "     context       0.85      0.73      0.79        48\n",
      "\n",
      "   micro avg       0.79      0.75      0.77       141\n",
      "   macro avg       0.78      0.75      0.76       141\n",
      "weighted avg       0.79      0.75      0.77       141\n",
      " samples avg       0.84      0.82      0.78       141\n",
      "\n",
      "\n",
      "CV F1-Score (micro): 0.8844 ¬± 0.0625\n",
      "\n",
      " accuracy : 0.7171 ¬± 0.0625\n",
      "Hamming Loss: 0.28444444444444444\n",
      "\n",
      "============================================================\n",
      "Optimisation de RandomForest\n",
      "============================================================\n",
      "Fitting 10 folds for each of 4 candidates, totalling 40 fits\n",
      "\n",
      "Meilleurs param√®tres: {'estimator__max_depth': None, 'estimator__min_samples_split': 5, 'estimator__n_estimators': 100}\n",
      "\n",
      "Performance sur le TEST SET:\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "       claim       0.83      0.83      0.83        54\n",
      "   reference       0.67      0.74      0.71        39\n",
      "     context       0.83      0.90      0.86        48\n",
      "\n",
      "   micro avg       0.79      0.83      0.81       141\n",
      "   macro avg       0.78      0.82      0.80       141\n",
      "weighted avg       0.79      0.83      0.81       141\n",
      " samples avg       0.83      0.87      0.81       141\n",
      "\n",
      "\n",
      "CV F1-Score (micro): 0.9133 ¬± 0.0507\n",
      "\n",
      " accuracy : 0.7836 ¬± 0.0507\n",
      "Hamming Loss: 0.24888888888888888\n",
      "\n",
      "============================================================\n",
      "Optimisation de SVM\n",
      "============================================================\n",
      "Fitting 10 folds for each of 6 candidates, totalling 60 fits\n",
      "\n",
      "Meilleurs param√®tres: {'estimator__C': 10, 'estimator__kernel': 'rbf'}\n",
      "\n",
      "Performance sur le TEST SET:\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "       claim       0.83      0.89      0.86        54\n",
      "   reference       0.64      0.82      0.72        39\n",
      "     context       0.74      0.96      0.84        48\n",
      "\n",
      "   micro avg       0.74      0.89      0.81       141\n",
      "   macro avg       0.74      0.89      0.80       141\n",
      "weighted avg       0.75      0.89      0.81       141\n",
      " samples avg       0.78      0.92      0.80       141\n",
      "\n",
      "\n",
      "CV F1-Score (micro): 0.8992 ¬± 0.0691\n",
      "\n",
      " accuracy : 0.7331 ¬± 0.0691\n",
      "Hamming Loss: 0.26222222222222225\n",
      "\n",
      "============================================================\n",
      "Optimisation de Naive Bayes\n",
      "============================================================\n",
      "Fitting 10 folds for each of 6 candidates, totalling 60 fits\n",
      "\n",
      "Meilleurs param√®tres: {'estimator__alpha': 1.0, 'estimator__fit_prior': True}\n",
      "\n",
      "Performance sur le TEST SET:\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "       claim       0.86      0.81      0.84        54\n",
      "   reference       0.60      0.62      0.61        39\n",
      "     context       0.68      1.00      0.81        48\n",
      "\n",
      "   micro avg       0.72      0.82      0.77       141\n",
      "   macro avg       0.71      0.81      0.75       141\n",
      "weighted avg       0.73      0.82      0.76       141\n",
      " samples avg       0.73      0.85      0.74       141\n",
      "\n",
      "\n",
      "CV F1-Score (micro): 0.8601 ¬± 0.0802\n",
      "\n",
      " accuracy : 0.6101 ¬± 0.0802\n",
      "Hamming Loss: 0.31555555555555553\n",
      "\n",
      "============================================================\n",
      "Optimisation de k-NN\n",
      "============================================================\n",
      "Fitting 10 folds for each of 12 candidates, totalling 120 fits\n",
      "\n",
      "Meilleurs param√®tres: {'estimator__metric': 'cosine', 'estimator__n_neighbors': 7, 'estimator__weights': 'distance'}\n",
      "\n",
      "Performance sur le TEST SET:\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "       claim       0.86      0.69      0.76        54\n",
      "   reference       0.57      0.64      0.60        39\n",
      "     context       0.67      0.96      0.79        48\n",
      "\n",
      "   micro avg       0.69      0.77      0.73       141\n",
      "   macro avg       0.70      0.76      0.72       141\n",
      "weighted avg       0.71      0.77      0.73       141\n",
      " samples avg       0.71      0.78      0.69       141\n",
      "\n",
      "\n",
      "CV F1-Score (micro): 0.8615 ¬± 0.0763\n",
      "\n",
      " accuracy : 0.6868 ¬± 0.0763\n",
      "Hamming Loss: 0.36\n",
      "\n",
      "============================================================\n",
      "Optimisation de Logistic Regression\n",
      "============================================================\n",
      "Fitting 10 folds for each of 6 candidates, totalling 60 fits\n",
      "\n",
      "Meilleurs param√®tres: {'estimator__C': 10, 'estimator__penalty': 'l1', 'estimator__solver': 'liblinear'}\n",
      "\n",
      "Performance sur le TEST SET:\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "       claim       0.82      0.83      0.83        54\n",
      "   reference       0.70      0.72      0.71        39\n",
      "     context       0.85      0.81      0.83        48\n",
      "\n",
      "   micro avg       0.79      0.79      0.79       141\n",
      "   macro avg       0.79      0.79      0.79       141\n",
      "weighted avg       0.80      0.79      0.79       141\n",
      " samples avg       0.84      0.85      0.80       141\n",
      "\n",
      "\n",
      "CV F1-Score (micro): 0.8933 ¬± 0.0545\n",
      "\n",
      " accuracy : 0.7433 ¬± 0.0545\n",
      "Hamming Loss: 0.2577777777777778\n",
      "\n",
      "============================================================\n",
      "Optimisation de Gradient Boosting\n",
      "============================================================\n",
      "Fitting 10 folds for each of 8 candidates, totalling 80 fits\n",
      "\n",
      "Meilleurs param√®tres: {'estimator__learning_rate': 0.1, 'estimator__max_depth': 5, 'estimator__n_estimators': 100}\n",
      "\n",
      "Performance sur le TEST SET:\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "       claim       0.83      0.93      0.88        54\n",
      "   reference       0.70      0.77      0.73        39\n",
      "     context       0.82      0.83      0.82        48\n",
      "\n",
      "   micro avg       0.79      0.85      0.82       141\n",
      "   macro avg       0.78      0.84      0.81       141\n",
      "weighted avg       0.79      0.85      0.82       141\n",
      " samples avg       0.83      0.90      0.82       141\n",
      "\n",
      "\n",
      "CV F1-Score (micro): 0.9086 ¬± 0.0476\n",
      "\n",
      " accuracy : 0.7696 ¬± 0.0476\n",
      "Hamming Loss: 0.23555555555555555\n"
     ]
    }
   ],
   "source": [
    "# ------------------------------------------------------------\n",
    "# SOLUTION FINALE POUR CLASSIFICATION MULTI-LABEL\n",
    "# Avec gestion du d√©s√©quilibre et validation crois√©e adapt√©e\n",
    "# ------------------------------------------------------------\n",
    "\"\"\"le Hamming loss est une m√©trique utilis√©e en classification multi-label (comme dans ton cas), \n",
    "et elle mesure la proportion de mauvaises pr√©dictions de labels par rapport au nombre total de labels.\n",
    "\n",
    "\"\"\"\n",
    "\n",
    "import numpy as np\n",
    "from sklearn.model_selection import train_test_split, KFold\n",
    "from sklearn.metrics import classification_report, f1_score\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.svm import LinearSVC\n",
    "from sklearn.multioutput import MultiOutputClassifier\n",
    "from sklearn.model_selection import GridSearchCV\n",
    "from scipy.sparse import vstack\n",
    "from collections import Counter\n",
    "from sklearn.ensemble import GradientBoostingClassifier\n",
    "\n",
    "\n",
    "# 1. Pr√©paration des donn√©es\n",
    "\n",
    "stop_words = set(stopwords.words('english'))\n",
    "custom_stop_words = {\"rt\", \"co\", \"amp\", \"via\"}\n",
    "#negations = {\"not\", \"no\", \"nor\", \"neither\", \"never\", \"none\"}\n",
    "stop_words = stop_words #- negations\n",
    "stop_words.update(custom_stop_words)\n",
    "lemmatizer = WordNetLemmatizer()\n",
    "\n",
    "def preprocess_tweet(tweet):\n",
    "    #tweet = re.sub(r\"@\\w+|\\W\", \" \", tweet)\n",
    "    tweet = re.sub(r\"(https?://\\S+)\", \" URL \", tweet)\n",
    "    tweet = tweet.lower()\n",
    "    tokens = word_tokenize(tweet)\n",
    "    tokens = [lemmatizer.lemmatize(word) for word in tokens if word not in stop_words]\n",
    "    return \" \".join(tokens)\n",
    "\n",
    "# 2. Chargement et pr√©paration\n",
    "sci_df = df[df['science_related'] == 1].copy()\n",
    "sci_df['cleaned_text'] = sci_df['text'].apply(preprocess_tweet)\n",
    "\n",
    "# 3. Cr√©ation des cibles multi-labels\n",
    "y = sci_df[['scientific_claim', 'scientific_reference', 'scientific_context']].values\n",
    "X = sci_df['cleaned_text']\n",
    "\n",
    "# 4. Vectorisation\n",
    "vectorizer = TfidfVectorizer(\n",
    "    ngram_range=(1, 3),\n",
    "    max_features=8000,\n",
    "    min_df=3,\n",
    "    max_df=0.85\n",
    ")\n",
    "X_vec = vectorizer.fit_transform(X)\n",
    "\n",
    "# 5. S√©paration train/test\n",
    "X_train, X_test, y_train, y_test = train_test_split(\n",
    "    X_vec, y, \n",
    "    test_size=0.2,\n",
    "    random_state=42\n",
    ")\n",
    "\n",
    "# 6. R√©√©chantillonnage manuel adapt√© au multi-label\n",
    "def multilabel_oversample(X, y, random_state=None):\n",
    "    np.random.seed(random_state)\n",
    "    \n",
    "    # Compter les occurrences de chaque combinaison de labels\n",
    "    unique_labels, counts = np.unique(y, axis=0, return_counts=True)\n",
    "    max_count = max(counts)\n",
    "    \n",
    "    resampled_X = []\n",
    "    resampled_y = []\n",
    "    \n",
    "    for label_combination, count in zip(unique_labels, counts):\n",
    "        indices = np.where((y == label_combination).all(axis=1))[0]\n",
    "        \n",
    "        # Sur√©chantillonnage seulement pour les classes minoritaires\n",
    "        if count < max_count:\n",
    "            n_to_add = max_count - count\n",
    "            selected = np.random.choice(indices, size=n_to_add, replace=True)\n",
    "            \n",
    "            resampled_X.append(X[selected])\n",
    "            resampled_y.append(y[selected])\n",
    "    \n",
    "    if resampled_X:\n",
    "        return vstack([X] + resampled_X), np.vstack([y] + resampled_y)\n",
    "    return X, y\n",
    "\n",
    "X_train_res, y_train_res = multilabel_oversample(X_train, y_train, random_state=42)\n",
    "\n",
    "# 7. Configuration des mod√®les\n",
    "models = {\n",
    "    \"LinearSVC\": {\n",
    "        \"model\": LinearSVC(dual=False, class_weight='balanced'),\n",
    "        \"params\": {\n",
    "            'estimator__C': [0.1, 1],\n",
    "            'estimator__penalty': ['l1', 'l2']\n",
    "        }\n",
    "    },\n",
    "    \"RandomForest\": {\n",
    "        \"model\": RandomForestClassifier(class_weight='balanced_subsample'),\n",
    "        \"params\": {\n",
    "            'estimator__n_estimators': [100],\n",
    "            'estimator__max_depth': [10, None],\n",
    "            'estimator__min_samples_split': [5, 10]\n",
    "        }\n",
    "    },\n",
    "    \"SVM\": {\n",
    "        \"model\": SVC(probability=True),\n",
    "        \"params\": {\n",
    "            'estimator__C': [0.1, 1, 10],\n",
    "            'estimator__kernel': ['linear', 'rbf']\n",
    "        }\n",
    "    },\n",
    "    \"Naive Bayes\": {\n",
    "        \"model\": MultinomialNB(),\n",
    "        \"params\": {\n",
    "            'estimator__alpha': [0.1, 0.5, 1.0],\n",
    "            'estimator__fit_prior': [True, False]\n",
    "        }\n",
    "    },\n",
    "    \"k-NN\": {\n",
    "        \"model\": KNeighborsClassifier(),\n",
    "        \"params\": {\n",
    "            'estimator__n_neighbors': [3, 5, 7],\n",
    "            'estimator__weights': ['uniform', 'distance'],\n",
    "            'estimator__metric': ['euclidean', 'cosine']\n",
    "        }\n",
    "    },\n",
    "    \n",
    "    \"Logistic Regression\": {\n",
    "        \"model\": LogisticRegression(max_iter=1000),\n",
    "        \"params\": {\n",
    "            'estimator__C': [0.1, 1, 10],\n",
    "            'estimator__penalty': ['l1', 'l2'],\n",
    "            'estimator__solver': ['liblinear']\n",
    "        }\n",
    "    },\n",
    "\n",
    "    \"Gradient Boosting\": {\n",
    "        \"model\": GradientBoostingClassifier(random_state=42),\n",
    "        \"params\": {\n",
    "            'estimator__n_estimators': [50, 100],\n",
    "            'estimator__learning_rate': [0.01, 0.1],\n",
    "            'estimator__max_depth': [3, 5]\n",
    "        }\n",
    "    }\n",
    "    \n",
    "}\n",
    "\n",
    "# 8. √âvaluation avec KFold standard (adapt√© au multi-label)\n",
    "kf = KFold(n_splits=10, shuffle=True, random_state=42)\n",
    "\n",
    "for name, config in models.items():\n",
    "    print(f\"\\n{'='*60}\")\n",
    "    print(f\"Optimisation de {name}\")\n",
    "    print(f\"{'='*60}\")\n",
    "    \n",
    "    model = MultiOutputClassifier(config['model'])\n",
    "    \n",
    "    grid = GridSearchCV(\n",
    "        model,\n",
    "        param_grid=config['params'],\n",
    "        cv=kf,\n",
    "        scoring='f1_micro',\n",
    "        n_jobs=-1,\n",
    "        verbose=1\n",
    "    )\n",
    "    \n",
    "    # Entra√Ænement sur donn√©es r√©√©chantillonn√©es\n",
    "    grid.fit(X_train_res, y_train_res)\n",
    "    \n",
    "    # √âvaluation sur test set original\n",
    "    y_pred = grid.predict(X_test)\n",
    "    \n",
    "    print(f\"\\nMeilleurs param√®tres: {grid.best_params_}\")\n",
    "    print(\"\\nPerformance sur le TEST SET:\")\n",
    "    print(classification_report(\n",
    "        y_test, y_pred,\n",
    "        target_names=['claim', 'reference', 'context'],\n",
    "        zero_division=0\n",
    "    ))\n",
    "    \n",
    "    # Scores de validation crois√©e\n",
    "    cv_scores = []\n",
    "    cv_score = []\n",
    "    for train_idx, test_idx in kf.split(X_train_res):\n",
    "        X_train_fold, X_val_fold = X_train_res[train_idx], X_train_res[test_idx]\n",
    "        y_train_fold, y_val_fold = y_train_res[train_idx], y_train_res[test_idx]\n",
    "        \n",
    "        model.set_params(**grid.best_params_)\n",
    "        model.fit(X_train_fold, y_train_fold)\n",
    "        y_pred_fold = model.predict(X_val_fold)\n",
    "        \n",
    "        # Calcul de l'accuracy au lieu du F1-score\n",
    "        cv_scores.append(accuracy_score(y_val_fold, y_pred_fold))\n",
    "        cv_score.append(f1_score(y_val_fold, y_pred_fold, average='micro'))\n",
    "    \n",
    "    print(f\"\\nCV F1-Score (micro): {np.mean(cv_score):.4f} ¬± {np.std(cv_scores):.4f}\")\n",
    "    print(f\"\\n accuracy : {np.mean(cv_scores):.4f} ¬± {np.std(cv_scores):.4f}\")\n",
    "    from sklearn.metrics import hamming_loss\n",
    "    print(\"Hamming Loss:\", hamming_loss(y_test, y_pred))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 84,
   "id": "f68d8dc8-1963-44be-b71d-a3f2cf1d1eaf",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Nombre d'erreurs: 36/75\n",
      "\n",
      "Exemples d'erreurs (5 premiers):\n",
      "\n",
      "Erreur 1:\n",
      "Texte: RT BBCScienceNews: Database helps plant 'right tree for the right place' https://t.co/NTNueGclGg\n",
      "V√©rit√©: claim=1.0, reference=1.0, context=1.0\n",
      "Pr√©dit: claim=1.0, reference=0.0, context=0.0\n",
      "----------------------------------------------------------------------------------------------------\n",
      "Erreur 2:\n",
      "Texte: What will digital life look like in a decade? Some predictions, from the optimistic to mind control http://t.co/q4F7Bice83\n",
      "V√©rit√©: claim=0.0, reference=1.0, context=1.0\n",
      "Pr√©dit: claim=1.0, reference=1.0, context=1.0\n",
      "----------------------------------------------------------------------------------------------------\n",
      "Erreur 3:\n",
      "Texte: When it comes to immigration, this is often left out of the conversation: The United States‚Äôs weak gun laws not only put American lives at risk but drive illegal gun trafficking and violence in Latin America. https://t.co/TIZrZLvWlu\n",
      "V√©rit√©: claim=1.0, reference=0.0, context=0.0\n",
      "Pr√©dit: claim=1.0, reference=0.0, context=1.0\n",
      "----------------------------------------------------------------------------------------------------\n",
      "Erreur 4:\n",
      "Texte: #Severe fibromyalgia symptoms linked to obesity, weight gain - https://t.co/NZG7iqYpas https://t.co/rnTKnmhL2V\n",
      "V√©rit√©: claim=1.0, reference=0.0, context=0.0\n",
      "Pr√©dit: claim=1.0, reference=1.0, context=1.0\n",
      "----------------------------------------------------------------------------------------------------\n",
      "Erreur 5:\n",
      "Texte: BioTechnology ‚Äì Efficient Stem Cells Can Grow in Animals https://t.co/W5yE3owX8R https://t.co/K24KwiCfXF\n",
      "V√©rit√©: claim=1.0, reference=0.0, context=1.0\n",
      "Pr√©dit: claim=1.0, reference=1.0, context=1.0\n",
      "----------------------------------------------------------------------------------------------------\n",
      "Erreur 6:\n",
      "Texte: Study: Marijuana Decriminalization Leads To Decreased Arrests, No Increase In Youth Use https://t.co/wzajo7UDKr\n",
      "V√©rit√©: claim=1.0, reference=1.0, context=1.0\n",
      "Pr√©dit: claim=1.0, reference=1.0, context=0.0\n",
      "----------------------------------------------------------------------------------------------------\n",
      "Erreur 7:\n",
      "Texte: 5 Surprising Ways Social Media is Affecting Healthcare ~ Social Media Frontiers: http://t.co/IxuQ4hfxw5\n",
      "V√©rit√©: claim=1.0, reference=0.0, context=0.0\n",
      "Pr√©dit: claim=1.0, reference=1.0, context=1.0\n",
      "----------------------------------------------------------------------------------------------------\n",
      "Erreur 8:\n",
      "Texte: Please Watch & share this powerful TEDx talk by Apollo Sevant that talks about how social & emotional learning & anti bullying workshops can lead to a reduction in school bullying & violence Video Here: https://t.co/6ckcpJGaUM\n",
      "V√©rit√©: claim=1.0, reference=0.0, context=0.0\n",
      "Pr√©dit: claim=1.0, reference=0.0, context=1.0\n",
      "----------------------------------------------------------------------------------------------------\n",
      "Erreur 9:\n",
      "Texte: @colinelves @d445467j @Dmzmhm20181 @pmdfoster @LDHMarketing @BorisJohnson @Nigel_Farage @brexitparty_uk @SamCoatesSky @EU_Commission The war was caused by indoctrination. Territorial indoctrination by Argentina almost led to war with Chile in 1978, and led to the Falklands War in 1982. Falklands - Argentina's Imaginary Territory (1 pg):- https://t.co/Vz1RYrCvHH\n",
      "V√©rit√©: claim=1.0, reference=1.0, context=1.0\n",
      "Pr√©dit: claim=1.0, reference=0.0, context=1.0\n",
      "----------------------------------------------------------------------------------------------------\n",
      "Erreur 10:\n",
      "Texte: From March 5 - 9 @FieldsInstitute's workshop on human-environment systems brings together researchers presenting advances on how to protect biodiversity, control species invasions, create resilient ecosystems, and much more. https://t.co/azsmfF9Zda https://t.co/kTUFZCOng0\n",
      "V√©rit√©: claim=0.0, reference=0.0, context=1.0\n",
      "Pr√©dit: claim=1.0, reference=1.0, context=1.0\n",
      "----------------------------------------------------------------------------------------------------\n",
      "Erreur 11:\n",
      "Texte: George #Boole Boolean logic inventor b OTD 1815: his Cork plaque https://t.co/y20LI0xGyQ #Boole200 @georgeboole200 https://t.co/3Ufu08KrrW\n",
      "V√©rit√©: claim=0.0, reference=0.0, context=1.0\n",
      "Pr√©dit: claim=0.0, reference=1.0, context=1.0\n",
      "----------------------------------------------------------------------------------------------------\n",
      "Erreur 12:\n",
      "Texte: Innovation - Infection Control - Mathematics supports a new way to classify viruses based on structure: https://t.co/LnptAW3iii\n",
      "V√©rit√©: claim=1.0, reference=1.0, context=1.0\n",
      "Pr√©dit: claim=0.0, reference=1.0, context=1.0\n",
      "----------------------------------------------------------------------------------------------------\n",
      "Erreur 13:\n",
      "Texte: I don‚Äôt like how much this is being promoted https://t.co/wc0FuvuJfT\n",
      "V√©rit√©: claim=0.0, reference=1.0, context=1.0\n",
      "Pr√©dit: claim=1.0, reference=1.0, context=1.0\n",
      "----------------------------------------------------------------------------------------------------\n",
      "Erreur 14:\n",
      "Texte: Freeman J. Dyson, a mathematical prodigy who left his mark on subatomic physics before gaining public renown as a writer and technological visionary, has died https://t.co/IDtWxjBUsk\n",
      "V√©rit√©: claim=0.0, reference=0.0, context=1.0\n",
      "Pr√©dit: claim=1.0, reference=1.0, context=1.0\n",
      "----------------------------------------------------------------------------------------------------\n",
      "Erreur 15:\n",
      "Texte: What satellites can tell us about how animals will fare in a changing climate - @NASA https://t.co/HJdka2hiSu #ClimateChange #biodiversity https://t.co/obxUKQ3U91\n",
      "V√©rit√©: claim=0.0, reference=1.0, context=1.0\n",
      "Pr√©dit: claim=1.0, reference=1.0, context=1.0\n",
      "----------------------------------------------------------------------------------------------------\n",
      "Erreur 16:\n",
      "Texte: How one state is becoming a leader in managing revenue volatility: https://t.co/rJ9qbNo6HB https://t.co/hgbaqkMQOO\n",
      "V√©rit√©: claim=0.0, reference=1.0, context=1.0\n",
      "Pr√©dit: claim=0.0, reference=1.0, context=0.0\n",
      "----------------------------------------------------------------------------------------------------\n",
      "Erreur 17:\n",
      "Texte: This looks like a great opportunity to get research in WASH published, with mentorship available too. Special issue of the H2open journal. https://t.co/DLBrcQx9QB @IWAPublishing #washtwitter #wash #sanitation #water #hygiene #SDGs\n",
      "V√©rit√©: claim=0.0, reference=0.0, context=1.0\n",
      "Pr√©dit: claim=1.0, reference=1.0, context=1.0\n",
      "----------------------------------------------------------------------------------------------------\n",
      "Erreur 18:\n",
      "Texte: VIDEO: Dr. Keith Flaherty talks about the genetics of cancer, #targetedtherapy, #clinicaltrials, precision medicine https://t.co/oFjd4YEEZe\n",
      "V√©rit√©: claim=0.0, reference=0.0, context=1.0\n",
      "Pr√©dit: claim=1.0, reference=1.0, context=1.0\n",
      "----------------------------------------------------------------------------------------------------\n",
      "Erreur 19:\n",
      "Texte: .@Oprah Nuclear weapons are a real, deadly problem that entire popul. of world must deal with http://t.co/6SEM76ud2c #SaveFukuChildren\n",
      "V√©rit√©: claim=1.0, reference=0.0, context=0.0\n",
      "Pr√©dit: claim=1.0, reference=1.0, context=1.0\n",
      "----------------------------------------------------------------------------------------------------\n",
      "Erreur 20:\n",
      "Texte: Q: Is there a connection between shellfish allergies and iodine? - Wilderness Medical Associates International https://t.co/4TM0FP7GQb\n",
      "V√©rit√©: claim=1.0, reference=0.0, context=0.0\n",
      "Pr√©dit: claim=1.0, reference=1.0, context=1.0\n",
      "----------------------------------------------------------------------------------------------------\n",
      "Erreur 21:\n",
      "Texte: Lupus Research Institute Awards $1-Million Grants to Discover What Causes Lupus http://t.co/aXopNmLyI7\n",
      "V√©rit√©: claim=0.0, reference=0.0, context=1.0\n",
      "Pr√©dit: claim=0.0, reference=1.0, context=1.0\n",
      "----------------------------------------------------------------------------------------------------\n",
      "Erreur 22:\n",
      "Texte: The effect of climate change on iceberg production by Greenland glaciers http://t.co/CtnPhcmD7c\n",
      "V√©rit√©: claim=0.0, reference=1.0, context=1.0\n",
      "Pr√©dit: claim=1.0, reference=1.0, context=1.0\n",
      "----------------------------------------------------------------------------------------------------\n",
      "Erreur 23:\n",
      "Texte: Highway noise deters communication between birds https://t.co/dB9He0XZhk\n",
      "V√©rit√©: claim=1.0, reference=1.0, context=1.0\n",
      "Pr√©dit: claim=0.0, reference=1.0, context=1.0\n",
      "----------------------------------------------------------------------------------------------------\n",
      "Erreur 24:\n",
      "Texte: Delaying hip fracture surgery for 1 day tied to slightly higher mortality risk https://t.co/YrMEfiUZJy @JAMA_current https://t.co/KvM67kV1pu\n",
      "V√©rit√©: claim=1.0, reference=1.0, context=1.0\n",
      "Pr√©dit: claim=0.0, reference=1.0, context=1.0\n",
      "----------------------------------------------------------------------------------------------------\n",
      "Erreur 25:\n",
      "Texte: @TexCIS @FoxNewsSunday 'Researchers found ‚Äúno evidence that urban protests reignited Covid-19‚Äîcities which had protests saw an increase in social distancing behavior for the overall population‚Äîleading to ‚Äúmodest evidence of a small longer-run case growth decline.‚Äù' https://t.co/7eVBgYs9Jj\n",
      "V√©rit√©: claim=1.0, reference=1.0, context=1.0\n",
      "Pr√©dit: claim=1.0, reference=0.0, context=0.0\n",
      "----------------------------------------------------------------------------------------------------\n",
      "Erreur 26:\n",
      "Texte: Why might women feel temperature differently to men?: Why might women feel temperature differently to men? http://t.co/vXGjFO55aw\n",
      "V√©rit√©: claim=1.0, reference=1.0, context=1.0\n",
      "Pr√©dit: claim=1.0, reference=0.0, context=0.0\n",
      "----------------------------------------------------------------------------------------------------\n",
      "Erreur 27:\n",
      "Texte: This story will tear you apart: The Last Person You‚Äôd Expect to Die in Childbirth https://t.co/2gGPOVUiv9\n",
      "V√©rit√©: claim=0.0, reference=1.0, context=1.0\n",
      "Pr√©dit: claim=1.0, reference=1.0, context=1.0\n",
      "----------------------------------------------------------------------------------------------------\n",
      "Erreur 28:\n",
      "Texte: Strawberries May Help Prevent Type 2 Diabetes, Says Study https://t.co/sUBh7GEGCV\n",
      "V√©rit√©: claim=1.0, reference=1.0, context=1.0\n",
      "Pr√©dit: claim=1.0, reference=0.0, context=0.0\n",
      "----------------------------------------------------------------------------------------------------\n",
      "Erreur 29:\n",
      "Texte: Posted blog with @schweitzer_va on @JAMAInternalMed study on a bundle to improve outcome in patients with community-acquired pneumonia. https://t.co/NSFsadd35G\n",
      "V√©rit√©: claim=0.0, reference=1.0, context=1.0\n",
      "Pr√©dit: claim=1.0, reference=1.0, context=1.0\n",
      "----------------------------------------------------------------------------------------------------\n",
      "Erreur 30:\n",
      "Texte: coumadin decreased sex drive men http://t.co/zfRNn4MMc6\n",
      "V√©rit√©: claim=1.0, reference=0.0, context=0.0\n",
      "Pr√©dit: claim=1.0, reference=1.0, context=1.0\n",
      "----------------------------------------------------------------------------------------------------\n",
      "Erreur 31:\n",
      "Texte: Durable, washable textile coating can repel viruses: New research could lead to safely reusable PPE https://t.co/KUyvMq5MRd\n",
      "V√©rit√©: claim=1.0, reference=1.0, context=1.0\n",
      "Pr√©dit: claim=0.0, reference=0.0, context=0.0\n",
      "----------------------------------------------------------------------------------------------------\n",
      "Erreur 32:\n",
      "Texte: Weed: Good for the Bones? YES! by Stephanie Pappas, Live Science Contributor http://t.co/L4pERvnmmo http://t.co/pPBGZwZLBG\n",
      "V√©rit√©: claim=1.0, reference=1.0, context=1.0\n",
      "Pr√©dit: claim=1.0, reference=0.0, context=1.0\n",
      "----------------------------------------------------------------------------------------------------\n",
      "Erreur 33:\n",
      "Texte: European markets are a better investment than US markets, analyst says https://t.co/APHnvJROtc #Stock #Market https://t.co/ykYySyP5vH\n",
      "V√©rit√©: claim=1.0, reference=0.0, context=0.0\n",
      "Pr√©dit: claim=1.0, reference=1.0, context=1.0\n",
      "----------------------------------------------------------------------------------------------------\n",
      "Erreur 34:\n",
      "Texte: France: N. Mayer. ‚ÄúBring the Poor Back In! Inequalities, Welfare and Politics‚Äù. In: European Political Science 13.2 (2014), pp. 187-200. https://t.co/bU7aVSgwU1. https://t.co/zhcg3vGe8r\n",
      "V√©rit√©: claim=0.0, reference=1.0, context=1.0\n",
      "Pr√©dit: claim=0.0, reference=0.0, context=1.0\n",
      "----------------------------------------------------------------------------------------------------\n",
      "Erreur 35:\n",
      "Texte: #ASH19 Paper No: 2723 Sun 1/7/19 6-8 PM #BPDCN Commonly Presents in the Setting of Prior or Concomitant Hematologic Malignancies: Patient Characteristics and Outcomes in the Rapidly Evolving Modern Targeted Therapy Era @JoeKhouryMD @doctorpemm @DrPhyuPAung2 @NitinJainMD\n",
      "V√©rit√©: claim=1.0, reference=1.0, context=1.0\n",
      "Pr√©dit: claim=1.0, reference=0.0, context=0.0\n",
      "----------------------------------------------------------------------------------------------------\n",
      "Erreur 36:\n",
      "Texte: Today the El Paso Department of Health reports 20 COVID-19 deaths. While this is the highest single-day increase, the deaths did not occur on the same day. https://t.co/p6NmrYpR0U\n",
      "V√©rit√©: claim=1.0, reference=0.0, context=0.0\n",
      "Pr√©dit: claim=1.0, reference=0.0, context=1.0\n",
      "----------------------------------------------------------------------------------------------------\n"
     ]
    }
   ],
   "source": [
    "# 1. R√©cup√©rer les indices originaux du test set\n",
    "_, test_indices = train_test_split(\n",
    "    range(len(sci_df)), \n",
    "    test_size=0.2, \n",
    "    random_state=42\n",
    ")\n",
    "\n",
    "# 2. Obtenir les textes originaux des tweets de test\n",
    "original_texts = sci_df.iloc[test_indices]['text'].values\n",
    "\n",
    "# 3. Identifier le meilleur mod√®le (Gradient Boosting dans votre cas)\n",
    "best_model_name = \"Gradient Boosting\"\n",
    "best_model = None\n",
    "\n",
    "for name, config in models.items():\n",
    "    if name == best_model_name:\n",
    "        model = MultiOutputClassifier(config['model'])\n",
    "        grid = GridSearchCV(model, config['params'], cv=kf, scoring='f1_micro')\n",
    "        grid.fit(X_train_res, y_train_res)\n",
    "        best_model = grid.best_estimator_\n",
    "        break\n",
    "\n",
    "# 4. Faire les pr√©dictions\n",
    "y_pred = best_model.predict(X_test)\n",
    "\n",
    "# 5. Identifier les erreurs CORRECTEMENT\n",
    "errors = []\n",
    "for i in range(len(y_test)):\n",
    "    if not np.array_equal(y_test[i], y_pred[i]):\n",
    "        errors.append({\n",
    "            'text': original_texts[i],  # Utilisation directe de l'index i\n",
    "            'true_claim': y_test[i][0],\n",
    "            'pred_claim': y_pred[i][0],\n",
    "            'true_reference': y_test[i][1], \n",
    "            'pred_reference': y_pred[i][1],\n",
    "            'true_context': y_test[i][2],\n",
    "            'pred_context': y_pred[i][2]\n",
    "        })\n",
    "\n",
    "# 6. Afficher les r√©sultats\n",
    "print(f\"\\nNombre d'erreurs: {len(errors)}/{len(y_test)}\")\n",
    "print(\"\\nExemples d'erreurs (5 premiers):\\n\")\n",
    "\n",
    "for i, error in enumerate(errors[:36]):\n",
    "    print(f\"Erreur {i+1}:\")\n",
    "    print(f\"Texte: {error['text']}\")\n",
    "    print(f\"V√©rit√©: claim={error['true_claim']}, reference={error['true_reference']}, context={error['true_context']}\")\n",
    "    print(f\"Pr√©dit: claim={error['pred_claim']}, reference={error['pred_reference']}, context={error['pred_context']}\")\n",
    "    print(\"-\"*100)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f0a9b8de-6027-4413-af8d-97bb66906351",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
