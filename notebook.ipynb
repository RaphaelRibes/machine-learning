{
 "cells": [
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "# Groupe 3 : Projet Machine Learning",
   "id": "1581c0c09fb635a9"
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "## Introduction",
   "id": "d8dc866d014797b1"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "import sklearn as sk\n",
    "import nltk"
   ],
   "id": "4fed691c69e344e6",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "dataset = pd.read_csv('scitweets_export.tsv', sep='\\t')\n",
    "dataset.head()"
   ],
   "id": "a6148ebdfced50a3",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "not_scientific = dataset.where(dataset['science_related'] == 0)\n",
    "scientific = dataset.where(dataset['science_related'] == 1)\n",
    "\n",
    "fig, ax = plt.subplots()\n",
    "ax.bar(['Not scientific', 'Scientific'], [not_scientific['tweet_id'].count(), scientific['tweet_id'].count()])\n",
    "plt.show()"
   ],
   "id": "a45705f54c099d6d",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "fig, ax = plt.subplots()\n",
    "ax.bar(['Claim', \"Reference\", \"Context\"],\n",
    "       [scientific.where(scientific['scientific_claim'] == 1)['tweet_id'].count(),\n",
    "        scientific.where(scientific['scientific_reference'] == 1)['tweet_id'].count(),\n",
    "        scientific.where(scientific['scientific_context'] == 1)['tweet_id'].count()])\n",
    "plt.show()"
   ],
   "id": "e0cc5917ce742473",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "",
   "id": "1d6e4e87df9607d5"
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "## Prétraitement",
   "id": "da9b469664d2c318"
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "On met tout en minuscule",
   "id": "2fc91b5a9f75e225"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "dataset['text'] = dataset['text'].apply(lambda x: x.lower())\n",
    "dataset.head()"
   ],
   "id": "3e2a74021f77a44",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": [
    "### Lemmatization\n",
    "\n",
    "download the packages"
   ],
   "id": "4b1cf00fcd39f0d6"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "nltk.download('wordnet')\n",
    "nltk.download('punkt')\n",
    "nltk.download('averaged_perceptron_tagger')\n",
    "nltk.download('stopwords')"
   ],
   "id": "f599d6e5487636c2",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "Remove the stopwords but not the negation words or the words that are important for the sentiment analysis",
   "id": "6f30142c21fae7ac"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "from nltk.corpus import stopwords\n",
    "\n",
    "stop_words = set(stopwords.words('english'))\n",
    "work_to_keep = ['no', 'not', 'nor', 'too', 'very', 'against', 'but', 'don', 'don\\'t', 'ain', 'aren', 'aren\\'t', 'couldn', 'couldn\\'t', 'didn', 'didn\\'t', 'doesn', 'doesn\\'t', 'hadn', 'hadn\\'t', 'hasn', 'hasn\\'t', 'haven', 'haven\\'t', 'isn', 'isn\\'t', 'mightn', 'mightn\\'t', 'mustn', 'mustn\\'t', 'needn', 'needn\\'t', 'shan', 'shan\\'t', 'shouldn', 'shouldn\\'t', 'wasn', 'wasn\\'t', 'weren', 'weren\\'t', 'won', 'won\\'t', 'wouldn', 'wouldn\\'t']\n",
    "\n",
    "for word in work_to_keep:\n",
    "    stop_words.remove(word)\n",
    "\n",
    "dataset['text'] = dataset['text'].apply(lambda x: ' '.join([word for word in x.split() if word not in stop_words]))\n",
    "dataset.head()"
   ],
   "id": "fbaae54d6ab9d695",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "Remove the links",
   "id": "d9819c7be1016348"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "for index, line in dataset.iterrows():\n",
    "    splitted = line['text'].split(' ')\n",
    "    new_text = []\n",
    "    for word in splitted:\n",
    "        if 'http' in word or 'www' in word:\n",
    "            continue\n",
    "        new_text.append(word)\n",
    "    dataset.at[index, 'text'] = ' '.join(new_text)\n",
    "\n",
    "dataset.head()"
   ],
   "id": "ecdb39bfc5c5418",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "Transform the emojis into text",
   "id": "2cc84b127835d4d8"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "import emoji\n",
    "\n",
    "# Parcourir les indices et modifier directement les valeurs dans le DataFrame\n",
    "for index, line in dataset.iterrows():\n",
    "    dataset.at[index, 'text'] = emoji.demojize(line['text'])\n",
    "\n",
    "# Afficher les premières lignes du DataFrame pour vérifier les modifications\n",
    "dataset.head()"
   ],
   "id": "de904ef265cf4e9",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "Remove the non-alphabetic characters except the #",
   "id": "4ab5fd359e7bc5a8"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "for index, line in dataset.iterrows():\n",
    "    splitted = line['text'].split(' ')\n",
    "    new_text = []\n",
    "    for word in splitted:\n",
    "        new_word = \"\"\n",
    "        for l in word:\n",
    "            if l.isalpha() or l in ['#', \"'\", '?', '!'] :\n",
    "                new_word += l\n",
    "            else:\n",
    "                new_word += ' '\n",
    "        new_text.append(new_word)\n",
    "    dataset.at[index, 'text'] = ' '.join(new_text)\n",
    "\n",
    "dataset.head()"
   ],
   "id": "508ed1f78f44bb63",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "Remove multiple spaces",
   "id": "b525f40fc197f78f"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "for index, line in dataset.iterrows():\n",
    "    dataset.at[index, 'text'] = ' '.join(line['text'].split())\n",
    "\n",
    "dataset.head()"
   ],
   "id": "ed37a5d9fd942b29",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": "\n",
   "id": "1ead8d24849ca2fd",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": [
    "## Vectorisation des textes\n",
    "\n",
    "(TF-IDF)"
   ],
   "id": "cfa40f6df98a88b6"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "from sklearn.model_selection import train_test_split\n",
    "from imblearn.over_sampling import SMOTE\n",
    "\n",
    "# Diviser les données en ensembles d'entraînement et de test\n",
    "X = dataset['text']  # Utiliser uniquement la colonne 'text' comme feature\n",
    "y_binary = dataset['science_related']  # Étiquette binaire (scientifique ou non)\n",
    "\n",
    "# Diviser les données AVANT la vectorisation\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y_binary, test_size=0.2, random_state=42)\n",
    "\n",
    "# Vectorisation : apprendre le vocabulaire et les poids TF-IDF uniquement sur les données d'entraînement\n",
    "vectorizer = TfidfVectorizer(max_features=100000)\n",
    "X_train_vec = vectorizer.fit_transform(X_train)  # Apprentissage et transformation sur l'entraînement\n",
    "\n",
    "# Appliquer la transformation aux données de test (sans réapprendre)\n",
    "X_test_vec = vectorizer.transform(X_test)"
   ],
   "id": "9936e3f56adf03db",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "TFID",
   "id": "f3377529f82e681b"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "from sklearn.decomposition import PCA\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "# Réduire la dimension à 2D avec PCA\n",
    "pca = PCA(n_components=2)\n",
    "X_train_pca = pca.fit_transform(X_train_vec.toarray())  # Convertir la matrice creuse en tableau dense\n",
    "\n",
    "# Visualiser les données en 2D\n",
    "plt.figure(figsize=(10, 6))\n",
    "plt.scatter(X_train_pca[:, 0], X_train_pca[:, 1], c=y_train, cmap='viridis', alpha=0.6)\n",
    "plt.colorbar(label='science_related')\n",
    "plt.title('Visualisation des tweets vectorisés (PCA 2D)')\n",
    "plt.xlabel('Composante principale 1')\n",
    "plt.ylabel('Composante principale 2')\n",
    "plt.show()"
   ],
   "id": "a06b28974e0f108c",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "TSNE",
   "id": "7fdef72b8c0ae020"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "from sklearn.manifold import TSNE\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "# Réduire la dimension à 2D avec t-SNE\n",
    "tsne = TSNE(n_components=2, random_state=42, perplexity=30)\n",
    "X_train_tsne = tsne.fit_transform(X_train_vec.toarray())\n",
    "\n",
    "# Visualiser les données en 2D\n",
    "plt.figure(figsize=(10, 6))\n",
    "plt.scatter(X_train_tsne[:, 0], X_train_tsne[:, 1], c=y_train, cmap='viridis', alpha=0.6)\n",
    "plt.colorbar(label='science_related')\n",
    "plt.title('Visualisation des tweets vectorisés (t-SNE 2D)')\n",
    "plt.xlabel('Composante t-SNE 1')\n",
    "plt.ylabel('Composante t-SNE 2')\n",
    "plt.show()"
   ],
   "id": "9169b66463a84aa4",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "## Choix des modèles",
   "id": "6595339af79426d6"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": "",
   "id": "d5aa4c36f7838737",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "",
   "id": "bbccb86184b486ba"
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
